{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparing surrogate models\n\n\nTim Head, July 2016.\nReformatted by Holger Nahrstaedt 2020\n\n.. currentmodule:: skopt\n\nBayesian optimization or sequential model-based optimization uses a surrogate\nmodel to model the expensive to evaluate function `func`. There are several\nchoices for what kind of surrogate model to use. This notebook compares the\nperformance of:\n\n* gaussian processes,\n* extra trees, and\n* random forests\n\nas surrogate models. A purely random optimization strategy is also used as\na baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\nimport numpy as np\nnp.random.seed(123)\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Toy model\n=========\n\nWe will use the :class:`benchmarks.branin` function as toy model for the expensive function.\nIn a real world application this function would be unknown and expensive\nto evaluate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skopt.benchmarks import branin as _branin\n\ndef branin(x, noise_level=0.):\n    return _branin(x) + noise_level * np.random.randn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import LogNorm\n\n\ndef plot_branin():\n    fig, ax = plt.subplots()\n\n    x1_values = np.linspace(-5, 10, 100)\n    x2_values = np.linspace(0, 15, 100)\n    x_ax, y_ax = np.meshgrid(x1_values, x2_values)\n    vals = np.c_[x_ax.ravel(), y_ax.ravel()]\n    fx = np.reshape([branin(val) for val in vals], (100, 100))\n\n    cm = ax.pcolormesh(x_ax, y_ax, fx,\n                       norm=LogNorm(vmin=fx.min(),\n                                    vmax=fx.max()))\n\n    minima = np.array([[-np.pi, 12.275], [+np.pi, 2.275], [9.42478, 2.475]])\n    ax.plot(minima[:, 0], minima[:, 1], \"r.\", markersize=14,\n            lw=0, label=\"Minima\")\n\n    cb = fig.colorbar(cm)\n    cb.set_label(\"f(x)\")\n\n    ax.legend(loc=\"best\", numpoints=1)\n\n    ax.set_xlabel(\"X1\")\n    ax.set_xlim([-5, 10])\n    ax.set_ylabel(\"X2\")\n    ax.set_ylim([0, 15])\n\n\nplot_branin()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This shows the value of the two-dimensional branin function and\nthe three minima.\n\n\nObjective\n=========\n\nThe objective of this example is to find one of these minima in as\nfew iterations as possible. One iteration is defined as one call\nto the :class:`benchmarks.branin` function.\n\nWe will evaluate each model several times using a different seed for the\nrandom number generator. Then compare the average performance of these\nmodels. This makes the comparison more robust against models that get\n\"lucky\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\nfrom skopt import gp_minimize, forest_minimize, dummy_minimize\n\nfunc = partial(branin, noise_level=2.0)\nbounds = [(-5.0, 10.0), (0.0, 15.0)]\nn_calls = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run(minimizer, n_iter=5):\n    return [minimizer(func, bounds, n_calls=n_calls, random_state=n)\n            for n in range(n_iter)]\n\n# Random search\ndummy_res = run(dummy_minimize)\n\n# Gaussian processes\ngp_res = run(gp_minimize)\n\n# Random forest\nrf_res = run(partial(forest_minimize, base_estimator=\"RF\"))\n\n# Extra trees\net_res = run(partial(forest_minimize, base_estimator=\"ET\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that this can take a few minutes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skopt.plots import plot_convergence\n\nplot = plot_convergence((\"dummy_minimize\", dummy_res),\n                        (\"gp_minimize\", gp_res),\n                        (\"forest_minimize('rf')\", rf_res),\n                        (\"forest_minimize('et)\", et_res),\n                        true_minimum=0.397887, yscale=\"log\")\n\nplot.legend(loc=\"best\", prop={'size': 6}, numpoints=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plot shows the value of the minimum found (y axis) as a function\nof the number of iterations performed so far (x axis). The dashed red line\nindicates the true value of the minimum of the :class:`benchmarks.branin` function.\n\nFor the first ten iterations all methods perform equally well as they all\nstart by creating ten random samples before fitting their respective model\nfor the first time. After iteration ten the next point at which\nto evaluate :class:`benchmarks.branin` is guided by the model, which is where differences\nstart to appear.\n\nEach minimizer only has access to noisy observations of the objective\nfunction, so as time passes (more iterations) it will start observing\nvalues that are below the true value simply because they are fluctuations.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}