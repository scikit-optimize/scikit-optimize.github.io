<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>skopt.learning API documentation</title>
    <meta name="description" content="Machine learning extensions for model-based optimization." />

  <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 0.9em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    padding-top: 0px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  }

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; }

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;

      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sa { color: #BA2121 } /* Literal.String.Affix */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .dl { color: #BA2121 } /* Literal.String.Delimiter */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .fm { color: #0000FF } /* Name.Function.Magic */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .vm { color: #19177C } /* Name.Variable.Magic */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
</head>
<body>
<a href="https://github.com/scikit-optimize/scikit-optimize"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <ul id="index">
    <li class="set"><h3><a href="https://scikit-optimize.github.io/">Index</a></h3></li>



    <li class="set"><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li class="mono">
        <span class="class_name"><a href="#skopt.learning.ExtraTreesRegressor">ExtraTreesRegressor</a></span>
        </li>
        <li class="mono">
        <span class="class_name"><a href="#skopt.learning.GaussianProcessRegressor">GaussianProcessRegressor</a></span>
        </li>
        <li class="mono">
        <span class="class_name"><a href="#skopt.learning.GradientBoostingQuantileRegressor">GradientBoostingQuantileRegressor</a></span>
        </li>
        <li class="mono">
        <span class="class_name"><a href="#skopt.learning.RandomForestRegressor">RandomForestRegressor</a></span>
        </li>
      </ul>
    </li>


    <li class="set"><h3><a href="https://scikit-optimize.github.io/"></a></h3>
    </li>

    <li class="set"><h3><a href="#">Notebooks</a></h3>
      <ul>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/ask-and-tell.html">Ask and tell</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/bayesian-optimization.html">Bayesian optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/hyperparameter-optimization.html">Hyperparameter optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/parallel-optimization.html">Parallel optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/sklearn-gridsearchcv-replacement.html">Sklearn gridsearchcv replacement</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/store-and-load-results.html">Store and load results</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/strategy-comparison.html">Strategy comparison</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/visualizing-results.html">Visualizing results</a></li>
      </ul>
    </li>
    </ul>
  </div>

    <article id="content">
          
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">skopt.learning</span> module</h1>
  <p>Machine learning extensions for model-based optimization.</p>
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning" class="source">
    <div class="codehilite"><pre><span></span><span class="sd">&quot;&quot;&quot;Machine learning extensions for model-based optimization.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">skgarden</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">skgarden</span> <span class="kn">import</span> <span class="n">ExtraTreesRegressor</span>

<span class="kn">from</span> <span class="nn">.gbrt</span> <span class="kn">import</span> <span class="n">GradientBoostingQuantileRegressor</span>
<span class="kn">from</span> <span class="nn">.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;RandomForestRegressor&quot;</span><span class="p">,</span>
           <span class="s2">&quot;ExtraTreesRegressor&quot;</span><span class="p">,</span>
           <span class="s2">&quot;GradientBoostingQuantileRegressor&quot;</span><span class="p">,</span>
           <span class="s2">&quot;GaussianProcessRegressor&quot;</span><span class="p">)</span>
</pre></div>

  </div>

  </header>



  <section id="section-items">


    <h2 class="section-title" id="header-classes">Classes</h2>
      
      <div class="item">
      <p id="skopt.learning.ExtraTreesRegressor" class="name">class <span class="ident">ExtraTreesRegressor</span></p>
      
  
    <div class="desc"><p>ExtraTreesRegressor that supports conditional standard deviation.</p>
<h2>Parameters</h2>
<p>n_estimators : integer, optional (default=10)
    The number of trees in the forest.</p>
<p>criterion : string, optional (default="mse")
    The function to measure the quality of a split. Supported criteria
    are "mse" for the mean squared error, which is equal to variance
    reduction as feature selection criterion, and "mae" for the mean
    absolute error.</p>
<p>max_features : int, float, string or None, optional (default="auto")
    The number of features to consider when looking for the best split:
    - If int, then consider <code>max_features</code> features at each split.
    - If float, then <code>max_features</code> is a percentage and
      <code>int(max_features * n_features)</code> features are considered at each
      split.
    - If "auto", then <code>max_features=n_features</code>.
    - If "sqrt", then <code>max_features=sqrt(n_features)</code>.
    - If "log2", then <code>max_features=log2(n_features)</code>.
    - If None, then <code>max_features=n_features</code>.
    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than <code>max_features</code> features.</p>
<p>max_depth : integer or None, optional (default=None)
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.</p>
<p>min_samples_split : int, float, optional (default=2)
    The minimum number of samples required to split an internal node:
    - If int, then consider <code>min_samples_split</code> as the minimum number.
    - If float, then <code>min_samples_split</code> is a percentage and
      <code>ceil(min_samples_split * n_samples)</code> are the minimum
      number of samples for each split.</p>
<p>min_samples_leaf : int, float, optional (default=1)
    The minimum number of samples required to be at a leaf node:
    - If int, then consider <code>min_samples_leaf</code> as the minimum number.
    - If float, then <code>min_samples_leaf</code> is a percentage and
      <code>ceil(min_samples_leaf * n_samples)</code> are the minimum
      number of samples for each node.</p>
<p>min_weight_fraction_leaf : float, optional (default=0.)
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.</p>
<p>max_leaf_nodes : int or None, optional (default=None)
    Grow trees with <code>max_leaf_nodes</code> in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.</p>
<p>min_impurity_decrease : float, optional (default=0.)
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.
    The weighted impurity decrease equation is the following::
        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)
    where <code>N</code> is the total number of samples, <code>N_t</code> is the number of
    samples at the current node, <code>N_t_L</code> is the number of samples in the
    left child, and <code>N_t_R</code> is the number of samples in the right child.
    <code>N</code>, <code>N_t</code>, <code>N_t_R</code> and <code>N_t_L</code> all refer to the weighted sum,
    if <code>sample_weight</code> is passed.</p>
<p>bootstrap : boolean, optional (default=True)
    Whether bootstrap samples are used when building trees.</p>
<p>oob_score : bool, optional (default=False)
    whether to use out-of-bag samples to estimate
    the R^2 on unseen data.</p>
<p>n_jobs : integer, optional (default=1)
    The number of jobs to run in parallel for both <code>fit</code> and <code>predict</code>.
    If -1, then the number of jobs is set to the number of cores.</p>
<p>random_state : int, RandomState instance or None, optional (default=None)
    If int, random_state is the seed used by the random number generator;
    If RandomState instance, random_state is the random number generator;
    If None, the random number generator is the RandomState instance used
    by <code>np.random</code>.</p>
<p>verbose : int, optional (default=0)
    Controls the verbosity of the tree building process.</p>
<p>warm_start : bool, optional (default=False)
    When set to <code>True</code>, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest.</p>
<h2>Attributes</h2>
<p>estimators_ : list of DecisionTreeRegressor
    The collection of fitted sub-estimators.</p>
<p>feature_importances_ : array of shape = [n_features]
    The feature importances (the higher, the more important the feature).</p>
<p>n_features_ : int
    The number of features when <code>fit</code> is performed.</p>
<p>n_outputs_ : int
    The number of outputs when <code>fit</code> is performed.</p>
<p>oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.</p>
<p>oob_prediction_ : array of shape = [n_samples]
    Prediction computed with out-of-bag estimate on the training set.</p>
<h2>Notes</h2>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code>max_depth</code>, <code>min_samples_leaf</code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code>max_features=n_features</code> and <code>bootstrap=False</code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code>random_state</code> has to be fixed.</p>
<h2>References</h2>
<p>.. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">ExtraTreesRegressor</span><span class="p">(</span><span class="n">_sk_ExtraTreesRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ExtraTreesRegressor that supports conditional standard deviation.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : integer, optional (default=10)</span>
<span class="sd">        The number of trees in the forest.</span>

<span class="sd">    criterion : string, optional (default=&quot;mse&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;mse&quot; for the mean squared error, which is equal to variance</span>
<span class="sd">        reduction as feature selection criterion, and &quot;mae&quot; for the mean</span>
<span class="sd">        absolute error.</span>

<span class="sd">    max_features : int, float, string or None, optional (default=&quot;auto&quot;)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>
<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a percentage and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=n_features`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>
<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_depth : integer or None, optional (default=None)</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>
<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a percentage and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node:</span>
<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a percentage and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>
<span class="sd">        The weighted impurity decrease equation is the following::</span>
<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>
<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>
<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">    bootstrap : boolean, optional (default=True)</span>
<span class="sd">        Whether bootstrap samples are used when building trees.</span>

<span class="sd">    oob_score : bool, optional (default=False)</span>
<span class="sd">        whether to use out-of-bag samples to estimate</span>
<span class="sd">        the R^2 on unseen data.</span>

<span class="sd">    n_jobs : integer, optional (default=1)</span>
<span class="sd">        The number of jobs to run in parallel for both `fit` and `predict`.</span>
<span class="sd">        If -1, then the number of jobs is set to the number of cores.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    verbose : int, optional (default=0)</span>
<span class="sd">        Controls the verbosity of the tree building process.</span>

<span class="sd">    warm_start : bool, optional (default=False)</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of DecisionTreeRegressor</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances (the higher, the more important the feature).</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>

<span class="sd">    oob_prediction_ : array of shape = [n_samples]</span>
<span class="sd">        Prediction computed with out-of-bag estimate on the training set.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The default values for the parameters controlling the size of the trees</span>
<span class="sd">    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and</span>
<span class="sd">    unpruned trees which can potentially be very large on some data sets. To</span>
<span class="sd">    reduce memory consumption, the complexity and size of the trees should be</span>
<span class="sd">    controlled by setting those parameter values.</span>
<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data,</span>
<span class="sd">    ``max_features=n_features`` and ``bootstrap=False``, if the improvement</span>
<span class="sd">    of the criterion is identical for several splits enumerated during the</span>
<span class="sd">    search of the best split. To obtain a deterministic behaviour during</span>
<span class="sd">    fitting, ``random_state`` has to be fixed.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] L. Breiman, &quot;Random Forests&quot;, Machine Learning, 45(1), 5-32, 2001.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">bootstrap</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">min_variance</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_variance</span> <span class="o">=</span> <span class="n">min_variance</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExtraTreesRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span>
            <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict continuous output for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape=(n_samples, n_features)</span>
<span class="sd">            Input data.</span>

<span class="sd">        return_std : boolean</span>
<span class="sd">            Whether or not to return the standard deviation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predictions : array-like of shape=(n_samples,)</span>
<span class="sd">            Predicted values for X. If criterion is set to &quot;mse&quot;,</span>
<span class="sd">            then `predictions[i] ~= mean(y | X[i])`.</span>

<span class="sd">        std : array-like of shape=(n_samples,)</span>
<span class="sd">            Standard deviation of `y` at `X`. If criterion</span>
<span class="sd">            is set to &quot;mse&quot;, then `std[i] ~= std(y | X[i])`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">ExtraTreesRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">!=</span> <span class="s2">&quot;mse&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Expected impurity to be &#39;mse&#39;, got </span><span class="si">%s</span><span class="s2"> instead&quot;</span>
                    <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>
            <span class="n">std</span> <span class="o">=</span> <span class="n">_return_std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_variance</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span>

        <span class="k">return</span> <span class="n">mean</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.learning.ExtraTreesRegressor">ExtraTreesRegressor</a></li>
          <li>sklearn.ensemble.forest.ExtraTreesRegressor</li>
          <li>sklearn.ensemble.forest.ForestRegressor</li>
          <li>abc.NewBase</li>
          <li>sklearn.ensemble.forest.BaseForest</li>
          <li>abc.NewBase</li>
          <li>sklearn.ensemble.base.BaseEnsemble</li>
          <li>sklearn.base.BaseEstimator</li>
          <li>sklearn.base.MetaEstimatorMixin</li>
          <li>sklearn.feature_selection.from_model._LearntSelectorMixin</li>
          <li>sklearn.base.TransformerMixin</li>
          <li>sklearn.base.RegressorMixin</li>
          <li>builtins.object</li>
          </ul>
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.learning.ExtraTreesRegressor.feature_importances_" class="name">var <span class="ident">feature_importances_</span></p>
            

            
  
    <div class="desc"><p>Return the feature importances (the higher, the more important the
   feature).</p>
<h2>Returns</h2>
<p>feature_importances_ : array, shape = [n_features]</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.learning.ExtraTreesRegressor.min_variance" class="name">var <span class="ident">min_variance</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
      
      <div class="item">
      <p id="skopt.learning.GaussianProcessRegressor" class="name">class <span class="ident">GaussianProcessRegressor</span></p>
      
  
    <div class="desc"><p>GaussianProcessRegressor that allows noise tunability.</p>
<p>The implementation is based on Algorithm 2.1 of Gaussian Processes
for Machine Learning (GPML) by Rasmussen and Williams.</p>
<p>In addition to standard scikit-learn estimator API,
GaussianProcessRegressor:</p>
<ul>
<li>allows prediction without prior fitting (based on the GP prior);</li>
<li>provides an additional method sample_y(X), which evaluates samples
     drawn from the GPR (prior or posterior) at given inputs;</li>
<li>exposes a method log_marginal_likelihood(theta), which can be used
     externally for other ways of selecting hyperparameters, e.g., via
     Markov chain Monte Carlo.</li>
</ul>
<h2>Parameters</h2>
<ul>
<li>
<p><code>kernel</code> [kernel object]:
    The kernel specifying the covariance function of the GP. If None is
    passed, the kernel "1.0 * RBF(1.0)" is used as default. Note that
    the kernel's hyperparameters are optimized during fitting.</p>
</li>
<li>
<p><code>alpha</code> [float or array-like, optional (default: 1e-10)]:
    Value added to the diagonal of the kernel matrix during fitting.
    Larger values correspond to increased noise level in the observations
    and reduce potential numerical issue during fitting. If an array is
    passed, it must have the same number of entries as the data used for
    fitting and is used as datapoint-dependent noise level. Note that this
    is equivalent to adding a WhiteKernel with c=alpha. Allowing to specify
    the noise level directly as a parameter is mainly for convenience and
    for consistency with Ridge.</p>
</li>
<li>
<p><code>optimizer</code> [string or callable, optional (default: "fmin_l_bfgs_b")]:
    Can either be one of the internally supported optimizers for optimizing
    the kernel's parameters, specified by a string, or an externally
    defined optimizer passed as a callable. If a callable is passed, it
    must have the signature::</p>
<div class="codehilite"><pre><span></span>def optimizer(obj_func, initial_theta, bounds):
    # * &#39;obj_func&#39; is the objective function to be maximized, which
    #   takes the hyperparameters theta as parameter and an
    #   optional flag eval_gradient, which determines if the
    #   gradient is returned additionally to the function value
    # * &#39;initial_theta&#39;: the initial value for theta, which can be
    #   used by local optimizers
    # * &#39;bounds&#39;: the bounds on the values of theta
    ....
    # Returned are the best found hyperparameters theta and
    # the corresponding value of the target function.
    return theta_opt, func_min
</pre></div>


<p>Per default, the 'fmin_l_bfgs_b' algorithm from scipy.optimize
is used. If None is passed, the kernel's parameters are kept fixed.
Available internal optimizers are::</p>
<div class="codehilite"><pre><span></span>&#39;fmin_l_bfgs_b&#39;
</pre></div>


</li>
<li>
<p><code>n_restarts_optimizer</code> [int, optional (default: 0)]:
    The number of restarts of the optimizer for finding the kernel's
    parameters which maximize the log-marginal likelihood. The first run
    of the optimizer is performed from the kernel's initial parameters,
    the remaining ones (if any) from thetas sampled log-uniform randomly
    from the space of allowed theta-values. If greater than 0, all bounds
    must be finite. Note that n_restarts_optimizer == 0 implies that one
    run is performed.</p>
</li>
<li>
<p><code>normalize_y</code> [boolean, optional (default: False)]:
    Whether the target values y are normalized, i.e., the mean of the
    observed target values become zero. This parameter should be set to
    True if the target values' mean is expected to differ considerable from
    zero. When enabled, the normalization effectively modifies the GP's
    prior based on the data, which contradicts the likelihood principle;
    normalization is thus disabled per default.</p>
</li>
<li>
<p><code>copy_X_train</code> [bool, optional (default: True)]:
    If True, a persistent copy of the training data is stored in the
    object. Otherwise, just a reference to the training data is stored,
    which might cause predictions to change if the data is modified
    externally.</p>
</li>
<li>
<p><code>random_state</code> [integer or numpy.RandomState, optional]:
    The generator used to initialize the centers. If an integer is
    given, it fixes the seed. Defaults to the global numpy random
    number generator.</p>
</li>
<li>
<p><code>noise</code> [string, "gaussian", optional]:
    If set to "gaussian", then it is assumed that <code>y</code> is a noisy
    estimate of <code>f(x)</code> where the noise is gaussian.</p>
</li>
</ul>
<h2>Attributes</h2>
<ul>
<li>
<p><code>X_train_</code> [array-like, shape = (n_samples, n_features)]:
    Feature values in training data (also required for prediction)</p>
</li>
<li>
<p><code>y_train_</code> [array-like, shape = (n_samples, [n_output_dims])]:
    Target values in training data (also required for prediction)</p>
</li>
<li>
<p><code>kernel_</code> [kernel object]:
    The kernel used for prediction. The structure of the kernel is the
    same as the one passed as parameter but with optimized hyperparameters</p>
</li>
<li>
<p><code>L_</code> [array-like, shape = (n_samples, n_samples)]:
    Lower-triangular Cholesky decomposition of the kernel in <code>X_train_</code></p>
</li>
<li>
<p><code>alpha_</code> [array-like, shape = (n_samples,)]:
    Dual coefficients of training data points in kernel space</p>
</li>
<li>
<p><code>log_marginal_likelihood_value_</code> [float]:
    The log-marginal-likelihood of <code>self.kernel_.theta</code></p>
</li>
<li>
<p><code>noise_</code> [float]:
    Estimate of the gaussian noise. Useful only when noise is set to
    "gaussian".</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GaussianProcessRegressor', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GaussianProcessRegressor" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">GaussianProcessRegressor</span><span class="p">(</span><span class="n">sk_GaussianProcessRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    GaussianProcessRegressor that allows noise tunability.</span>

<span class="sd">    The implementation is based on Algorithm 2.1 of Gaussian Processes</span>
<span class="sd">    for Machine Learning (GPML) by Rasmussen and Williams.</span>

<span class="sd">    In addition to standard scikit-learn estimator API,</span>
<span class="sd">    GaussianProcessRegressor:</span>

<span class="sd">       * allows prediction without prior fitting (based on the GP prior);</span>
<span class="sd">       * provides an additional method sample_y(X), which evaluates samples</span>
<span class="sd">         drawn from the GPR (prior or posterior) at given inputs;</span>
<span class="sd">       * exposes a method log_marginal_likelihood(theta), which can be used</span>
<span class="sd">         externally for other ways of selecting hyperparameters, e.g., via</span>
<span class="sd">         Markov chain Monte Carlo.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `kernel` [kernel object]:</span>
<span class="sd">        The kernel specifying the covariance function of the GP. If None is</span>
<span class="sd">        passed, the kernel &quot;1.0 * RBF(1.0)&quot; is used as default. Note that</span>
<span class="sd">        the kernel&#39;s hyperparameters are optimized during fitting.</span>

<span class="sd">    * `alpha` [float or array-like, optional (default: 1e-10)]:</span>
<span class="sd">        Value added to the diagonal of the kernel matrix during fitting.</span>
<span class="sd">        Larger values correspond to increased noise level in the observations</span>
<span class="sd">        and reduce potential numerical issue during fitting. If an array is</span>
<span class="sd">        passed, it must have the same number of entries as the data used for</span>
<span class="sd">        fitting and is used as datapoint-dependent noise level. Note that this</span>
<span class="sd">        is equivalent to adding a WhiteKernel with c=alpha. Allowing to specify</span>
<span class="sd">        the noise level directly as a parameter is mainly for convenience and</span>
<span class="sd">        for consistency with Ridge.</span>

<span class="sd">    * `optimizer` [string or callable, optional (default: &quot;fmin_l_bfgs_b&quot;)]:</span>
<span class="sd">        Can either be one of the internally supported optimizers for optimizing</span>
<span class="sd">        the kernel&#39;s parameters, specified by a string, or an externally</span>
<span class="sd">        defined optimizer passed as a callable. If a callable is passed, it</span>
<span class="sd">        must have the signature::</span>

<span class="sd">            def optimizer(obj_func, initial_theta, bounds):</span>
<span class="sd">                # * &#39;obj_func&#39; is the objective function to be maximized, which</span>
<span class="sd">                #   takes the hyperparameters theta as parameter and an</span>
<span class="sd">                #   optional flag eval_gradient, which determines if the</span>
<span class="sd">                #   gradient is returned additionally to the function value</span>
<span class="sd">                # * &#39;initial_theta&#39;: the initial value for theta, which can be</span>
<span class="sd">                #   used by local optimizers</span>
<span class="sd">                # * &#39;bounds&#39;: the bounds on the values of theta</span>
<span class="sd">                ....</span>
<span class="sd">                # Returned are the best found hyperparameters theta and</span>
<span class="sd">                # the corresponding value of the target function.</span>
<span class="sd">                return theta_opt, func_min</span>

<span class="sd">        Per default, the &#39;fmin_l_bfgs_b&#39; algorithm from scipy.optimize</span>
<span class="sd">        is used. If None is passed, the kernel&#39;s parameters are kept fixed.</span>
<span class="sd">        Available internal optimizers are::</span>

<span class="sd">            &#39;fmin_l_bfgs_b&#39;</span>

<span class="sd">    * `n_restarts_optimizer` [int, optional (default: 0)]:</span>
<span class="sd">        The number of restarts of the optimizer for finding the kernel&#39;s</span>
<span class="sd">        parameters which maximize the log-marginal likelihood. The first run</span>
<span class="sd">        of the optimizer is performed from the kernel&#39;s initial parameters,</span>
<span class="sd">        the remaining ones (if any) from thetas sampled log-uniform randomly</span>
<span class="sd">        from the space of allowed theta-values. If greater than 0, all bounds</span>
<span class="sd">        must be finite. Note that n_restarts_optimizer == 0 implies that one</span>
<span class="sd">        run is performed.</span>

<span class="sd">    * `normalize_y` [boolean, optional (default: False)]:</span>
<span class="sd">        Whether the target values y are normalized, i.e., the mean of the</span>
<span class="sd">        observed target values become zero. This parameter should be set to</span>
<span class="sd">        True if the target values&#39; mean is expected to differ considerable from</span>
<span class="sd">        zero. When enabled, the normalization effectively modifies the GP&#39;s</span>
<span class="sd">        prior based on the data, which contradicts the likelihood principle;</span>
<span class="sd">        normalization is thus disabled per default.</span>

<span class="sd">    * `copy_X_train` [bool, optional (default: True)]:</span>
<span class="sd">        If True, a persistent copy of the training data is stored in the</span>
<span class="sd">        object. Otherwise, just a reference to the training data is stored,</span>
<span class="sd">        which might cause predictions to change if the data is modified</span>
<span class="sd">        externally.</span>

<span class="sd">    * `random_state` [integer or numpy.RandomState, optional]:</span>
<span class="sd">        The generator used to initialize the centers. If an integer is</span>
<span class="sd">        given, it fixes the seed. Defaults to the global numpy random</span>
<span class="sd">        number generator.</span>

<span class="sd">    * `noise` [string, &quot;gaussian&quot;, optional]:</span>
<span class="sd">        If set to &quot;gaussian&quot;, then it is assumed that `y` is a noisy</span>
<span class="sd">        estimate of `f(x)` where the noise is gaussian.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X_train_` [array-like, shape = (n_samples, n_features)]:</span>
<span class="sd">        Feature values in training data (also required for prediction)</span>

<span class="sd">    * `y_train_` [array-like, shape = (n_samples, [n_output_dims])]:</span>
<span class="sd">        Target values in training data (also required for prediction)</span>

<span class="sd">    * `kernel_` [kernel object]:</span>
<span class="sd">        The kernel used for prediction. The structure of the kernel is the</span>
<span class="sd">        same as the one passed as parameter but with optimized hyperparameters</span>

<span class="sd">    * `L_` [array-like, shape = (n_samples, n_samples)]:</span>
<span class="sd">        Lower-triangular Cholesky decomposition of the kernel in ``X_train_``</span>

<span class="sd">    * `alpha_` [array-like, shape = (n_samples,)]:</span>
<span class="sd">        Dual coefficients of training data points in kernel space</span>

<span class="sd">    * `log_marginal_likelihood_value_` [float]:</span>
<span class="sd">        The log-marginal-likelihood of ``self.kernel_.theta``</span>

<span class="sd">    * `noise_` [float]:</span>
<span class="sd">        Estimate of the gaussian noise. Useful only when noise is set to</span>
<span class="sd">        &quot;gaussian&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;fmin_l_bfgs_b&quot;</span><span class="p">,</span> <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">normalize_y</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">copy_X_train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">=</span> <span class="n">noise</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GaussianProcessRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
            <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="n">n_restarts_optimizer</span><span class="p">,</span>
            <span class="n">normalize_y</span><span class="o">=</span><span class="n">normalize_y</span><span class="p">,</span> <span class="n">copy_X_train</span><span class="o">=</span><span class="n">copy_X_train</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit Gaussian process regression model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [array-like, shape = (n_samples, n_features)]:</span>
<span class="sd">            Training data</span>

<span class="sd">        * `y` [array-like, shape = (n_samples, [n_output_dims])]:</span>
<span class="sd">            Target values</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        * `self`:</span>
<span class="sd">            Returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">!=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected noise to be &#39;gaussian&#39;, got </span><span class="si">%s</span><span class="s2">&quot;</span>
                             <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">constant_value_bounds</span><span class="o">=</span><span class="s2">&quot;fixed&quot;</span><span class="p">)</span> \
                          <span class="o">*</span> <span class="n">RBF</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">length_scale_bounds</span><span class="o">=</span><span class="s2">&quot;fixed&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">+</span> <span class="n">WhiteKernel</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">+</span> <span class="n">WhiteKernel</span><span class="p">(</span>
                <span class="n">noise_level</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span> <span class="n">noise_level_bounds</span><span class="o">=</span><span class="s2">&quot;fixed&quot;</span>
            <span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GaussianProcessRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">noise_</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">:</span>
            <span class="c1"># The noise component of this kernel should be set to zero</span>
            <span class="c1"># while estimating K(X_test, X_test)</span>
            <span class="c1"># Note that the term K(X, X) should include the noise but</span>
            <span class="c1"># this (K(X, X))^-1y is precomputed as the attribute `alpha_`.</span>
            <span class="c1"># (Notice the underscore).</span>
            <span class="c1"># This has been described in Eq 2.24 of</span>
            <span class="c1"># http://www.gaussianprocess.org/gpml/chapters/RW2.pdf</span>
            <span class="c1"># Hence this hack</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">,</span> <span class="n">WhiteKernel</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">noise_level</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">white_present</span><span class="p">,</span> <span class="n">white_param</span> <span class="o">=</span> <span class="n">_param_for_white_kernel_in_Sum</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">)</span>

                <span class="c1"># This should always be true. Just in case.</span>
                <span class="k">if</span> <span class="n">white_present</span><span class="p">:</span>
                    <span class="n">noise_kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()[</span><span class="n">white_param</span><span class="p">]</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">noise_</span> <span class="o">=</span> <span class="n">noise_kernel</span><span class="o">.</span><span class="n">noise_level</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span>
                        <span class="o">**</span><span class="p">{</span><span class="n">white_param</span><span class="p">:</span> <span class="n">WhiteKernel</span><span class="p">(</span><span class="n">noise_level</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)})</span>

        <span class="c1"># Precompute arrays needed at prediction</span>
        <span class="n">L_inv</span> <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K_inv_</span> <span class="o">=</span> <span class="n">L_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L_inv</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

        <span class="c1"># Fix deprecation warning #462</span>
        <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">19</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_train_mean_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_train_mean</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_train_mean_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train_mean</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">return_mean_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">return_std_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict output for X.</span>

<span class="sd">        In addition to the mean of the predictive distribution, also its</span>
<span class="sd">        standard deviation (return_std=True) or covariance (return_cov=True),</span>
<span class="sd">        the gradient of the mean and the standard-deviation with respect to X</span>
<span class="sd">        can be optionally provided.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [array-like, shape = (n_samples, n_features)]:</span>
<span class="sd">            Query points where the GP is evaluated.</span>

<span class="sd">        * `return_std` [bool, default: False]:</span>
<span class="sd">            If True, the standard-deviation of the predictive distribution at</span>
<span class="sd">            the query points is returned along with the mean.</span>

<span class="sd">        * `return_cov` [bool, default: False]:</span>
<span class="sd">            If True, the covariance of the joint predictive distribution at</span>
<span class="sd">            the query points is returned along with the mean.</span>

<span class="sd">        * `return_mean_grad` [bool, default: False]:</span>
<span class="sd">            Whether or not to return the gradient of the mean.</span>
<span class="sd">            Only valid when X is a single point.</span>

<span class="sd">        * `return_std_grad` [bool, default: False]:</span>
<span class="sd">            Whether or not to return the gradient of the std.</span>
<span class="sd">            Only valid when X is a single point.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        * `y_mean` [array, shape = (n_samples, [n_output_dims]):</span>
<span class="sd">            Mean of predictive distribution a query points</span>

<span class="sd">        * `y_std` [array, shape = (n_samples,), optional]:</span>
<span class="sd">            Standard deviation of predictive distribution at query points.</span>
<span class="sd">            Only returned when return_std is True.</span>

<span class="sd">        * `y_cov` [array, shape = (n_samples, n_samples), optional]:</span>
<span class="sd">            Covariance of joint predictive distribution a query points.</span>
<span class="sd">            Only returned when return_cov is True.</span>

<span class="sd">        * `y_mean_grad` [shape = (n_samples, n_features)]:</span>
<span class="sd">            The gradient of the predicted mean</span>

<span class="sd">        * `y_std_grad` [shape = (n_samples, n_features)]:</span>
<span class="sd">            The gradient of the predicted std.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">return_std</span> <span class="ow">and</span> <span class="n">return_cov</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Not returning standard deviation of predictions when &quot;</span>
                <span class="s2">&quot;returning full covariance.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_std_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">return_std</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Not returning std_gradient without returning &quot;</span>
                <span class="s2">&quot;the std.&quot;</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">return_mean_grad</span> <span class="ow">or</span> <span class="n">return_std_grad</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Not implemented for n_samples &gt; 1&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;X_train_&quot;</span><span class="p">):</span>  <span class="c1"># Not fit; predict based on GP prior</span>
            <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">return_cov</span><span class="p">:</span>
                <span class="n">y_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_cov</span>
            <span class="k">elif</span> <span class="n">return_std</span><span class="p">:</span>
                <span class="n">y_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">y_var</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">y_mean</span>

        <span class="k">else</span><span class="p">:</span>  <span class="c1"># Predict based on GP posterior</span>
            <span class="n">K_trans</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train_</span><span class="p">)</span>
            <span class="n">y_mean</span> <span class="o">=</span> <span class="n">K_trans</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>    <span class="c1"># Line 4 (y_mean = f_star)</span>
            <span class="n">y_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train_mean_</span> <span class="o">+</span> <span class="n">y_mean</span>  <span class="c1"># undo normal.</span>

            <span class="k">if</span> <span class="n">return_cov</span><span class="p">:</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">cho_solve</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">L_</span><span class="p">,</span> <span class="bp">True</span><span class="p">),</span> <span class="n">K_trans</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>  <span class="c1"># Line 5</span>
                <span class="n">y_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">K_trans</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>   <span class="c1"># Line 6</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_cov</span>

            <span class="k">elif</span> <span class="n">return_std</span><span class="p">:</span>
                <span class="n">K_inv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_inv_</span>

                <span class="c1"># Compute variance of predictive distribution</span>
                <span class="n">y_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="n">y_var</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ki,kj,ij-&gt;k&quot;</span><span class="p">,</span> <span class="n">K_trans</span><span class="p">,</span> <span class="n">K_trans</span><span class="p">,</span> <span class="n">K_inv</span><span class="p">)</span>

                <span class="c1"># Check if any of the variances is negative because of</span>
                <span class="c1"># numerical issues. If yes: set the variance to 0.</span>
                <span class="n">y_var_negative</span> <span class="o">=</span> <span class="n">y_var</span> <span class="o">&lt;</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">y_var_negative</span><span class="p">):</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Predicted variances smaller than 0. &quot;</span>
                                  <span class="s2">&quot;Setting those variances to 0.&quot;</span><span class="p">)</span>
                    <span class="n">y_var</span><span class="p">[</span><span class="n">y_var_negative</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
                <span class="n">y_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">y_var</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_mean_grad</span><span class="p">:</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">gradient_x</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train_</span><span class="p">)</span>
                <span class="n">grad_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">return_std_grad</span><span class="p">:</span>
                    <span class="n">grad_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">y_std</span><span class="p">,</span> <span class="n">grad_std</span><span class="p">):</span>
                        <span class="n">grad_std</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_trans</span><span class="p">,</span>
                                           <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_inv</span><span class="p">,</span> <span class="n">grad</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">y_std</span>
                    <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">grad_mean</span><span class="p">,</span> <span class="n">grad_std</span>

                <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">grad_mean</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">grad_mean</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">y_mean</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.learning.GaussianProcessRegressor">GaussianProcessRegressor</a></li>
          <li>sklearn.gaussian_process.gpr.GaussianProcessRegressor</li>
          <li>sklearn.base.BaseEstimator</li>
          <li>sklearn.base.RegressorMixin</li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="skopt.learning.GaussianProcessRegressor.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, kernel=None, alpha=0.0, optimizer=&#39;fmin_l_bfgs_b&#39;, n_restarts_optimizer=0, normalize_y=False, copy_X_train=True, random_state=None, noise=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GaussianProcessRegressor.__init__', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GaussianProcessRegressor.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
             <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;fmin_l_bfgs_b&quot;</span><span class="p">,</span> <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="n">normalize_y</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">copy_X_train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">=</span> <span class="n">noise</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GaussianProcessRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
        <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="n">n_restarts_optimizer</span><span class="p">,</span>
        <span class="n">normalize_y</span><span class="o">=</span><span class="n">normalize_y</span><span class="p">,</span> <span class="n">copy_X_train</span><span class="o">=</span><span class="n">copy_X_train</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.GaussianProcessRegressor.fit">
    <p>def <span class="ident">fit</span>(</p><p>self, X, y)</p>
    </div>
    

    
  
    <div class="desc"><p>Fit Gaussian process regression model.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>X</code> [array-like, shape = (n_samples, n_features)]:
    Training data</p>
</li>
<li>
<p><code>y</code> [array-like, shape = (n_samples, [n_output_dims])]:
    Target values</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li><code>self</code>:
    Returns an instance of self.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GaussianProcessRegressor.fit', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GaussianProcessRegressor.fit" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit Gaussian process regression model.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape = (n_samples, n_features)]:</span>
<span class="sd">        Training data</span>
<span class="sd">    * `y` [array-like, shape = (n_samples, [n_output_dims])]:</span>
<span class="sd">        Target values</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `self`:</span>
<span class="sd">        Returns an instance of self.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">!=</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected noise to be &#39;gaussian&#39;, got </span><span class="si">%s</span><span class="s2">&quot;</span>
                         <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">constant_value_bounds</span><span class="o">=</span><span class="s2">&quot;fixed&quot;</span><span class="p">)</span> \
                      <span class="o">*</span> <span class="n">RBF</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">length_scale_bounds</span><span class="o">=</span><span class="s2">&quot;fixed&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">==</span> <span class="s2">&quot;gaussian&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">+</span> <span class="n">WhiteKernel</span><span class="p">()</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">+</span> <span class="n">WhiteKernel</span><span class="p">(</span>
            <span class="n">noise_level</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span> <span class="n">noise_level_bounds</span><span class="o">=</span><span class="s2">&quot;fixed&quot;</span>
        <span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GaussianProcessRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">noise_</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">:</span>
        <span class="c1"># The noise component of this kernel should be set to zero</span>
        <span class="c1"># while estimating K(X_test, X_test)</span>
        <span class="c1"># Note that the term K(X, X) should include the noise but</span>
        <span class="c1"># this (K(X, X))^-1y is precomputed as the attribute `alpha_`.</span>
        <span class="c1"># (Notice the underscore).</span>
        <span class="c1"># This has been described in Eq 2.24 of</span>
        <span class="c1"># http://www.gaussianprocess.org/gpml/chapters/RW2.pdf</span>
        <span class="c1"># Hence this hack</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">,</span> <span class="n">WhiteKernel</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">noise_level</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">white_present</span><span class="p">,</span> <span class="n">white_param</span> <span class="o">=</span> <span class="n">_param_for_white_kernel_in_Sum</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">)</span>
            <span class="c1"># This should always be true. Just in case.</span>
            <span class="k">if</span> <span class="n">white_present</span><span class="p">:</span>
                <span class="n">noise_kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()[</span><span class="n">white_param</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">noise_</span> <span class="o">=</span> <span class="n">noise_kernel</span><span class="o">.</span><span class="n">noise_level</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span>
                    <span class="o">**</span><span class="p">{</span><span class="n">white_param</span><span class="p">:</span> <span class="n">WhiteKernel</span><span class="p">(</span><span class="n">noise_level</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)})</span>
    <span class="c1"># Precompute arrays needed at prediction</span>
    <span class="n">L_inv</span> <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">K_inv_</span> <span class="o">=</span> <span class="n">L_inv</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">L_inv</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="c1"># Fix deprecation warning #462</span>
    <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="mi">19</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train_mean_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_train_mean</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_train_mean_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train_mean</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.GaussianProcessRegressor.predict">
    <p>def <span class="ident">predict</span>(</p><p>self, X, return_std=False, return_cov=False, return_mean_grad=False, return_std_grad=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Predict output for X.</p>
<p>In addition to the mean of the predictive distribution, also its
standard deviation (return_std=True) or covariance (return_cov=True),
the gradient of the mean and the standard-deviation with respect to X
can be optionally provided.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>X</code> [array-like, shape = (n_samples, n_features)]:
    Query points where the GP is evaluated.</p>
</li>
<li>
<p><code>return_std</code> [bool, default: False]:
    If True, the standard-deviation of the predictive distribution at
    the query points is returned along with the mean.</p>
</li>
<li>
<p><code>return_cov</code> [bool, default: False]:
    If True, the covariance of the joint predictive distribution at
    the query points is returned along with the mean.</p>
</li>
<li>
<p><code>return_mean_grad</code> [bool, default: False]:
    Whether or not to return the gradient of the mean.
    Only valid when X is a single point.</p>
</li>
<li>
<p><code>return_std_grad</code> [bool, default: False]:
    Whether or not to return the gradient of the std.
    Only valid when X is a single point.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>y_mean</code> [array, shape = (n_samples, [n_output_dims]):
    Mean of predictive distribution a query points</p>
</li>
<li>
<p><code>y_std</code> [array, shape = (n_samples,), optional]:
    Standard deviation of predictive distribution at query points.
    Only returned when return_std is True.</p>
</li>
<li>
<p><code>y_cov</code> [array, shape = (n_samples, n_samples), optional]:
    Covariance of joint predictive distribution a query points.
    Only returned when return_cov is True.</p>
</li>
<li>
<p><code>y_mean_grad</code> [shape = (n_samples, n_features)]:
    The gradient of the predicted mean</p>
</li>
<li>
<p><code>y_std_grad</code> [shape = (n_samples, n_features)]:
    The gradient of the predicted std.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GaussianProcessRegressor.predict', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GaussianProcessRegressor.predict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">return_mean_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">return_std_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict output for X.</span>
<span class="sd">    In addition to the mean of the predictive distribution, also its</span>
<span class="sd">    standard deviation (return_std=True) or covariance (return_cov=True),</span>
<span class="sd">    the gradient of the mean and the standard-deviation with respect to X</span>
<span class="sd">    can be optionally provided.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape = (n_samples, n_features)]:</span>
<span class="sd">        Query points where the GP is evaluated.</span>
<span class="sd">    * `return_std` [bool, default: False]:</span>
<span class="sd">        If True, the standard-deviation of the predictive distribution at</span>
<span class="sd">        the query points is returned along with the mean.</span>
<span class="sd">    * `return_cov` [bool, default: False]:</span>
<span class="sd">        If True, the covariance of the joint predictive distribution at</span>
<span class="sd">        the query points is returned along with the mean.</span>
<span class="sd">    * `return_mean_grad` [bool, default: False]:</span>
<span class="sd">        Whether or not to return the gradient of the mean.</span>
<span class="sd">        Only valid when X is a single point.</span>
<span class="sd">    * `return_std_grad` [bool, default: False]:</span>
<span class="sd">        Whether or not to return the gradient of the std.</span>
<span class="sd">        Only valid when X is a single point.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `y_mean` [array, shape = (n_samples, [n_output_dims]):</span>
<span class="sd">        Mean of predictive distribution a query points</span>
<span class="sd">    * `y_std` [array, shape = (n_samples,), optional]:</span>
<span class="sd">        Standard deviation of predictive distribution at query points.</span>
<span class="sd">        Only returned when return_std is True.</span>
<span class="sd">    * `y_cov` [array, shape = (n_samples, n_samples), optional]:</span>
<span class="sd">        Covariance of joint predictive distribution a query points.</span>
<span class="sd">        Only returned when return_cov is True.</span>
<span class="sd">    * `y_mean_grad` [shape = (n_samples, n_features)]:</span>
<span class="sd">        The gradient of the predicted mean</span>
<span class="sd">    * `y_std_grad` [shape = (n_samples, n_features)]:</span>
<span class="sd">        The gradient of the predicted std.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">return_std</span> <span class="ow">and</span> <span class="n">return_cov</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Not returning standard deviation of predictions when &quot;</span>
            <span class="s2">&quot;returning full covariance.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_std_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">return_std</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Not returning std_gradient without returning &quot;</span>
            <span class="s2">&quot;the std.&quot;</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="p">(</span><span class="n">return_mean_grad</span> <span class="ow">or</span> <span class="n">return_std_grad</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Not implemented for n_samples &gt; 1&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;X_train_&quot;</span><span class="p">):</span>  <span class="c1"># Not fit; predict based on GP prior</span>
        <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">return_cov</span><span class="p">:</span>
            <span class="n">y_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_cov</span>
        <span class="k">elif</span> <span class="n">return_std</span><span class="p">:</span>
            <span class="n">y_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">y_var</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y_mean</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Predict based on GP posterior</span>
        <span class="n">K_trans</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train_</span><span class="p">)</span>
        <span class="n">y_mean</span> <span class="o">=</span> <span class="n">K_trans</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>    <span class="c1"># Line 4 (y_mean = f_star)</span>
        <span class="n">y_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train_mean_</span> <span class="o">+</span> <span class="n">y_mean</span>  <span class="c1"># undo normal.</span>
        <span class="k">if</span> <span class="n">return_cov</span><span class="p">:</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">cho_solve</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">L_</span><span class="p">,</span> <span class="bp">True</span><span class="p">),</span> <span class="n">K_trans</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>  <span class="c1"># Line 5</span>
            <span class="n">y_cov</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="n">K_trans</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>   <span class="c1"># Line 6</span>
            <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_cov</span>
        <span class="k">elif</span> <span class="n">return_std</span><span class="p">:</span>
            <span class="n">K_inv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">K_inv_</span>
            <span class="c1"># Compute variance of predictive distribution</span>
            <span class="n">y_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">y_var</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ki,kj,ij-&gt;k&quot;</span><span class="p">,</span> <span class="n">K_trans</span><span class="p">,</span> <span class="n">K_trans</span><span class="p">,</span> <span class="n">K_inv</span><span class="p">)</span>
            <span class="c1"># Check if any of the variances is negative because of</span>
            <span class="c1"># numerical issues. If yes: set the variance to 0.</span>
            <span class="n">y_var_negative</span> <span class="o">=</span> <span class="n">y_var</span> <span class="o">&lt;</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">y_var_negative</span><span class="p">):</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Predicted variances smaller than 0. &quot;</span>
                              <span class="s2">&quot;Setting those variances to 0.&quot;</span><span class="p">)</span>
                <span class="n">y_var</span><span class="p">[</span><span class="n">y_var_negative</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">y_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">y_var</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_mean_grad</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">gradient_x</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train_</span><span class="p">)</span>
            <span class="n">grad_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">return_std_grad</span><span class="p">:</span>
                <span class="n">grad_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">y_std</span><span class="p">,</span> <span class="n">grad_std</span><span class="p">):</span>
                    <span class="n">grad_std</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_trans</span><span class="p">,</span>
                                       <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_inv</span><span class="p">,</span> <span class="n">grad</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">y_std</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">grad_mean</span><span class="p">,</span> <span class="n">grad_std</span>
            <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span><span class="p">,</span> <span class="n">grad_mean</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">grad_mean</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">y_mean</span><span class="p">,</span> <span class="n">y_std</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">y_mean</span>
</pre></div>

  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.learning.GaussianProcessRegressor.noise" class="name">var <span class="ident">noise</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
      
      <div class="item">
      <p id="skopt.learning.GradientBoostingQuantileRegressor" class="name">class <span class="ident">GradientBoostingQuantileRegressor</span></p>
      
  
    <div class="desc"><p>Predict several quantiles with one estimator.</p>
<p>This is a wrapper around <code>GradientBoostingRegressor</code>'s quantile
regression that allows you to predict several <code>quantiles</code> in
one go.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>quantiles</code> [array-like]:
    Quantiles to predict. By default the 16, 50 and 84%
    quantiles are predicted.</p>
</li>
<li>
<p><code>base_estimator</code> [GradientBoostingRegressor instance or None (default)]:
    Quantile regressor used to make predictions. Only instances
    of <code>GradientBoostingRegressor</code> are supported. Use this to change
    the hyper-parameters of the estimator.</p>
</li>
<li>
<p><code>n_jobs</code> [int, default=1]:
    The number of jobs to run in parallel for <code>fit</code>.
    If -1, then the number of jobs is set to the number of cores.</p>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">GradientBoostingQuantileRegressor</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict several quantiles with one estimator.</span>

<span class="sd">    This is a wrapper around `GradientBoostingRegressor`&#39;s quantile</span>
<span class="sd">    regression that allows you to predict several `quantiles` in</span>
<span class="sd">    one go.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `quantiles` [array-like]:</span>
<span class="sd">        Quantiles to predict. By default the 16, 50 and 84%</span>
<span class="sd">        quantiles are predicted.</span>

<span class="sd">    * `base_estimator` [GradientBoostingRegressor instance or None (default)]:</span>
<span class="sd">        Quantile regressor used to make predictions. Only instances</span>
<span class="sd">        of `GradientBoostingRegressor` are supported. Use this to change</span>
<span class="sd">        the hyper-parameters of the estimator.</span>

<span class="sd">    * `n_jobs` [int, default=1]:</span>
<span class="sd">        The number of jobs to run in parallel for `fit`.</span>
<span class="sd">        If -1, then the number of jobs is set to the number of cores.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">],</span> <span class="n">base_estimator</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span> <span class="o">=</span> <span class="n">quantiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="o">=</span> <span class="n">base_estimator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit one regressor for each quantile.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">            Training vectors, where `n_samples` is the number of samples</span>
<span class="sd">            and `n_features` is the number of features.</span>

<span class="sd">        * `y` [array-like, shape=(n_samples,)]:</span>
<span class="sd">            Target values (real numbers in regression)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">base_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;base_estimator has to be of type&#39;</span>
                                 <span class="s1">&#39; GradientBoostingRegressor.&#39;</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">base_estimator</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;base_estimator has to use quantile&#39;</span>
                                 <span class="s1">&#39; loss not </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">base_estimator</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

        <span class="c1"># The predictions for different quantiles should be sorted.</span>
        <span class="c1"># Therefore each of the regressors need the same seed.</span>
        <span class="n">base_estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">regressors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="p">:</span>
            <span class="n">regressor</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span>
            <span class="n">regressor</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>

            <span class="n">regressors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regressor</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;threading&#39;</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_parallel_fit</span><span class="p">)(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">regressor</span> <span class="ow">in</span> <span class="n">regressors</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">return_quantiles</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict.</span>

<span class="sd">        Predict `X` at every quantile if `return_std` is set to False.</span>
<span class="sd">        If `return_std` is set to True, then return the mean</span>
<span class="sd">        and the predicted standard deviation, which is approximated as</span>
<span class="sd">        the (0.84th quantile - 0.16th quantile) divided by 2.0</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">            where `n_samples` is the number of samples</span>
<span class="sd">            and `n_features` is the number of features.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">predicted_quantiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
            <span class="p">[</span><span class="n">rgr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">rgr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">return_quantiles</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">predicted_quantiles</span><span class="o">.</span><span class="n">T</span>

        <span class="k">elif</span> <span class="n">return_std</span><span class="p">:</span>
            <span class="n">std_quantiles</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">]</span>
            <span class="n">is_present_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">in1d</span><span class="p">(</span><span class="n">std_quantiles</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">is_present_mask</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;return_std works only if the quantiles during &quot;</span>
                    <span class="s2">&quot;instantiation include 0.16, 0.5 and 0.84&quot;</span><span class="p">)</span>
            <span class="n">low</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.16</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">high</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.84</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="p">((</span><span class="n">high</span> <span class="o">-</span> <span class="n">low</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">)</span>

        <span class="c1"># return the mean</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.learning.GradientBoostingQuantileRegressor">GradientBoostingQuantileRegressor</a></li>
          <li>sklearn.base.BaseEstimator</li>
          <li>sklearn.base.RegressorMixin</li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="skopt.learning.GradientBoostingQuantileRegressor.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, quantiles=[0.16, 0.5, 0.84], base_estimator=None, n_jobs=1, random_state=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor.__init__', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">],</span> <span class="n">base_estimator</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span> <span class="o">=</span> <span class="n">quantiles</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="o">=</span> <span class="n">base_estimator</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.GradientBoostingQuantileRegressor.fit">
    <p>def <span class="ident">fit</span>(</p><p>self, X, y)</p>
    </div>
    

    
  
    <div class="desc"><p>Fit one regressor for each quantile.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>X</code> [array-like, shape=(n_samples, n_features)]:
    Training vectors, where <code>n_samples</code> is the number of samples
    and <code>n_features</code> is the number of features.</p>
</li>
<li>
<p><code>y</code> [array-like, shape=(n_samples,)]:
    Target values (real numbers in regression)</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor.fit', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor.fit" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit one regressor for each quantile.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Training vectors, where `n_samples` is the number of samples</span>
<span class="sd">        and `n_features` is the number of features.</span>
<span class="sd">    * `y` [array-like, shape=(n_samples,)]:</span>
<span class="sd">        Target values (real numbers in regression)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">base_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;base_estimator has to be of type&#39;</span>
                             <span class="s1">&#39; GradientBoostingRegressor.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">base_estimator</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;base_estimator has to use quantile&#39;</span>
                             <span class="s1">&#39; loss not </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">base_estimator</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
    <span class="c1"># The predictions for different quantiles should be sorted.</span>
    <span class="c1"># Therefore each of the regressors need the same seed.</span>
    <span class="n">base_estimator</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">regressors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="p">:</span>
        <span class="n">regressor</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span>
        <span class="n">regressor</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
        <span class="n">regressors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regressor</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;threading&#39;</span><span class="p">)(</span>
        <span class="n">delayed</span><span class="p">(</span><span class="n">_parallel_fit</span><span class="p">)(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">regressor</span> <span class="ow">in</span> <span class="n">regressors</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.GradientBoostingQuantileRegressor.predict">
    <p>def <span class="ident">predict</span>(</p><p>self, X, return_std=False, return_quantiles=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Predict.</p>
<p>Predict <code>X</code> at every quantile if <code>return_std</code> is set to False.
If <code>return_std</code> is set to True, then return the mean
and the predicted standard deviation, which is approximated as
the (0.84th quantile - 0.16th quantile) divided by 2.0</p>
<h2>Parameters</h2>
<ul>
<li><code>X</code> [array-like, shape=(n_samples, n_features)]:
    where <code>n_samples</code> is the number of samples
    and <code>n_features</code> is the number of features.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor.predict', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor.predict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">return_quantiles</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict.</span>
<span class="sd">    Predict `X` at every quantile if `return_std` is set to False.</span>
<span class="sd">    If `return_std` is set to True, then return the mean</span>
<span class="sd">    and the predicted standard deviation, which is approximated as</span>
<span class="sd">    the (0.84th quantile - 0.16th quantile) divided by 2.0</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        where `n_samples` is the number of samples</span>
<span class="sd">        and `n_features` is the number of features.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predicted_quantiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
        <span class="p">[</span><span class="n">rgr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">rgr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">return_quantiles</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">predicted_quantiles</span><span class="o">.</span><span class="n">T</span>
    <span class="k">elif</span> <span class="n">return_std</span><span class="p">:</span>
        <span class="n">std_quantiles</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">]</span>
        <span class="n">is_present_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">in1d</span><span class="p">(</span><span class="n">std_quantiles</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">is_present_mask</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;return_std works only if the quantiles during &quot;</span>
                <span class="s2">&quot;instantiation include 0.16, 0.5 and 0.84&quot;</span><span class="p">)</span>
        <span class="n">low</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.16</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.84</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="p">((</span><span class="n">high</span> <span class="o">-</span> <span class="n">low</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">)</span>
    <span class="c1"># return the mean</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.learning.GradientBoostingQuantileRegressor.base_estimator" class="name">var <span class="ident">base_estimator</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.learning.GradientBoostingQuantileRegressor.n_jobs" class="name">var <span class="ident">n_jobs</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.learning.GradientBoostingQuantileRegressor.quantiles" class="name">var <span class="ident">quantiles</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.learning.GradientBoostingQuantileRegressor.random_state" class="name">var <span class="ident">random_state</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
      
      <div class="item">
      <p id="skopt.learning.RandomForestRegressor" class="name">class <span class="ident">RandomForestRegressor</span></p>
      
  
    <div class="desc"><p>RandomForestRegressor that supports conditional std computation.</p>
<h2>Parameters</h2>
<p>n_estimators : integer, optional (default=10)
    The number of trees in the forest.</p>
<p>criterion : string, optional (default="mse")
    The function to measure the quality of a split. Supported criteria
    are "mse" for the mean squared error, which is equal to variance
    reduction as feature selection criterion, and "mae" for the mean
    absolute error.</p>
<p>max_features : int, float, string or None, optional (default="auto")
    The number of features to consider when looking for the best split:
    - If int, then consider <code>max_features</code> features at each split.
    - If float, then <code>max_features</code> is a percentage and
      <code>int(max_features * n_features)</code> features are considered at each
      split.
    - If "auto", then <code>max_features=n_features</code>.
    - If "sqrt", then <code>max_features=sqrt(n_features)</code>.
    - If "log2", then <code>max_features=log2(n_features)</code>.
    - If None, then <code>max_features=n_features</code>.
    Note: the search for a split does not stop until at least one
    valid partition of the node samples is found, even if it requires to
    effectively inspect more than <code>max_features</code> features.</p>
<p>max_depth : integer or None, optional (default=None)
    The maximum depth of the tree. If None, then nodes are expanded until
    all leaves are pure or until all leaves contain less than
    min_samples_split samples.</p>
<p>min_samples_split : int, float, optional (default=2)
    The minimum number of samples required to split an internal node:
    - If int, then consider <code>min_samples_split</code> as the minimum number.
    - If float, then <code>min_samples_split</code> is a percentage and
      <code>ceil(min_samples_split * n_samples)</code> are the minimum
      number of samples for each split.</p>
<p>min_samples_leaf : int, float, optional (default=1)
    The minimum number of samples required to be at a leaf node:
    - If int, then consider <code>min_samples_leaf</code> as the minimum number.
    - If float, then <code>min_samples_leaf</code> is a percentage and
      <code>ceil(min_samples_leaf * n_samples)</code> are the minimum
      number of samples for each node.</p>
<p>min_weight_fraction_leaf : float, optional (default=0.)
    The minimum weighted fraction of the sum total of weights (of all
    the input samples) required to be at a leaf node. Samples have
    equal weight when sample_weight is not provided.</p>
<p>max_leaf_nodes : int or None, optional (default=None)
    Grow trees with <code>max_leaf_nodes</code> in best-first fashion.
    Best nodes are defined as relative reduction in impurity.
    If None then unlimited number of leaf nodes.</p>
<p>min_impurity_decrease : float, optional (default=0.)
    A node will be split if this split induces a decrease of the impurity
    greater than or equal to this value.
    The weighted impurity decrease equation is the following::
        N_t / N * (impurity - N_t_R / N_t * right_impurity
                            - N_t_L / N_t * left_impurity)
    where <code>N</code> is the total number of samples, <code>N_t</code> is the number of
    samples at the current node, <code>N_t_L</code> is the number of samples in the
    left child, and <code>N_t_R</code> is the number of samples in the right child.
    <code>N</code>, <code>N_t</code>, <code>N_t_R</code> and <code>N_t_L</code> all refer to the weighted sum,
    if <code>sample_weight</code> is passed.</p>
<p>bootstrap : boolean, optional (default=True)
    Whether bootstrap samples are used when building trees.</p>
<p>oob_score : bool, optional (default=False)
    whether to use out-of-bag samples to estimate
    the R^2 on unseen data.</p>
<p>n_jobs : integer, optional (default=1)
    The number of jobs to run in parallel for both <code>fit</code> and <code>predict</code>.
    If -1, then the number of jobs is set to the number of cores.</p>
<p>random_state : int, RandomState instance or None, optional (default=None)
    If int, random_state is the seed used by the random number generator;
    If RandomState instance, random_state is the random number generator;
    If None, the random number generator is the RandomState instance used
    by <code>np.random</code>.</p>
<p>verbose : int, optional (default=0)
    Controls the verbosity of the tree building process.</p>
<p>warm_start : bool, optional (default=False)
    When set to <code>True</code>, reuse the solution of the previous call to fit
    and add more estimators to the ensemble, otherwise, just fit a whole
    new forest.</p>
<h2>Attributes</h2>
<p>estimators_ : list of DecisionTreeRegressor
    The collection of fitted sub-estimators.</p>
<p>feature_importances_ : array of shape = [n_features]
    The feature importances (the higher, the more important the feature).</p>
<p>n_features_ : int
    The number of features when <code>fit</code> is performed.</p>
<p>n_outputs_ : int
    The number of outputs when <code>fit</code> is performed.</p>
<p>oob_score_ : float
    Score of the training dataset obtained using an out-of-bag estimate.</p>
<p>oob_prediction_ : array of shape = [n_samples]
    Prediction computed with out-of-bag estimate on the training set.</p>
<h2>Notes</h2>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code>max_depth</code>, <code>min_samples_leaf</code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.
The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code>max_features=n_features</code> and <code>bootstrap=False</code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code>random_state</code> has to be fixed.</p>
<h2>References</h2>
<p>.. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="n">_sk_RandomForestRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    RandomForestRegressor that supports conditional std computation.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : integer, optional (default=10)</span>
<span class="sd">        The number of trees in the forest.</span>

<span class="sd">    criterion : string, optional (default=&quot;mse&quot;)</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;mse&quot; for the mean squared error, which is equal to variance</span>
<span class="sd">        reduction as feature selection criterion, and &quot;mae&quot; for the mean</span>
<span class="sd">        absolute error.</span>

<span class="sd">    max_features : int, float, string or None, optional (default=&quot;auto&quot;)</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>
<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a percentage and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=n_features`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>
<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_depth : integer or None, optional (default=None)</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int, float, optional (default=2)</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>
<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a percentage and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">    min_samples_leaf : int, float, optional (default=1)</span>
<span class="sd">        The minimum number of samples required to be at a leaf node:</span>
<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a percentage and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">    min_weight_fraction_leaf : float, optional (default=0.)</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_leaf_nodes : int or None, optional (default=None)</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, optional (default=0.)</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>
<span class="sd">        The weighted impurity decrease equation is the following::</span>
<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>
<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>
<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">    bootstrap : boolean, optional (default=True)</span>
<span class="sd">        Whether bootstrap samples are used when building trees.</span>

<span class="sd">    oob_score : bool, optional (default=False)</span>
<span class="sd">        whether to use out-of-bag samples to estimate</span>
<span class="sd">        the R^2 on unseen data.</span>

<span class="sd">    n_jobs : integer, optional (default=1)</span>
<span class="sd">        The number of jobs to run in parallel for both `fit` and `predict`.</span>
<span class="sd">        If -1, then the number of jobs is set to the number of cores.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>

<span class="sd">    verbose : int, optional (default=0)</span>
<span class="sd">        Controls the verbosity of the tree building process.</span>

<span class="sd">    warm_start : bool, optional (default=False)</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of DecisionTreeRegressor</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    feature_importances_ : array of shape = [n_features]</span>
<span class="sd">        The feature importances (the higher, the more important the feature).</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>

<span class="sd">    oob_prediction_ : array of shape = [n_samples]</span>
<span class="sd">        Prediction computed with out-of-bag estimate on the training set.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The default values for the parameters controlling the size of the trees</span>
<span class="sd">    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and</span>
<span class="sd">    unpruned trees which can potentially be very large on some data sets. To</span>
<span class="sd">    reduce memory consumption, the complexity and size of the trees should be</span>
<span class="sd">    controlled by setting those parameter values.</span>
<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data,</span>
<span class="sd">    ``max_features=n_features`` and ``bootstrap=False``, if the improvement</span>
<span class="sd">    of the criterion is identical for several splits enumerated during the</span>
<span class="sd">    search of the best split. To obtain a deterministic behaviour during</span>
<span class="sd">    fitting, ``random_state`` has to be fixed.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] L. Breiman, &quot;Random Forests&quot;, Machine Learning, 45(1), 5-32, 2001.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">bootstrap</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">min_variance</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_variance</span> <span class="o">=</span> <span class="n">min_variance</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span>
            <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict continuous output for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array of shape = (n_samples, n_features)</span>
<span class="sd">            Input data.</span>

<span class="sd">        return_std : boolean</span>
<span class="sd">            Whether or not to return the standard deviation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        predictions : array-like of shape = (n_samples,)</span>
<span class="sd">            Predicted values for X. If criterion is set to &quot;mse&quot;,</span>
<span class="sd">            then `predictions[i] ~= mean(y | X[i])`.</span>

<span class="sd">        std : array-like of shape=(n_samples,)</span>
<span class="sd">            Standard deviation of `y` at `X`. If criterion</span>
<span class="sd">            is set to &quot;mse&quot;, then `std[i] ~= std(y | X[i])`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">!=</span> <span class="s2">&quot;mse&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Expected impurity to be &#39;mse&#39;, got </span><span class="si">%s</span><span class="s2"> instead&quot;</span>
                    <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>
            <span class="n">std</span> <span class="o">=</span> <span class="n">_return_std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_variance</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span>
        <span class="k">return</span> <span class="n">mean</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.learning.RandomForestRegressor">RandomForestRegressor</a></li>
          <li>sklearn.ensemble.forest.RandomForestRegressor</li>
          <li>sklearn.ensemble.forest.ForestRegressor</li>
          <li>abc.NewBase</li>
          <li>sklearn.ensemble.forest.BaseForest</li>
          <li>abc.NewBase</li>
          <li>sklearn.ensemble.base.BaseEnsemble</li>
          <li>sklearn.base.BaseEstimator</li>
          <li>sklearn.base.MetaEstimatorMixin</li>
          <li>sklearn.feature_selection.from_model._LearntSelectorMixin</li>
          <li>sklearn.base.TransformerMixin</li>
          <li>sklearn.base.RegressorMixin</li>
          <li>builtins.object</li>
          </ul>
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.learning.RandomForestRegressor.feature_importances_" class="name">var <span class="ident">feature_importances_</span></p>
            

            
  
    <div class="desc"><p>Return the feature importances (the higher, the more important the
   feature).</p>
<h2>Returns</h2>
<p>feature_importances_ : array, shape = [n_features]</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.learning.RandomForestRegressor.min_variance" class="name">var <span class="ident">min_variance</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
