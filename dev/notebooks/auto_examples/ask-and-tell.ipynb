{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Async optimization Loop\n\nBayesian optimization is used to tune parameters for walking robots or other\nexperiments that are not a simple (expensive) function call.\n\nTim Head, February 2017.\nReformatted by Holger Nahrstaedt 2020\n\n.. currentmodule:: skopt\n\nThey often follow a pattern a bit like this:\n\n1. ask for a new set of parameters\n2. walk to the experiment and program in the new parameters\n3. observe the outcome of running the experiment\n4. walk back to your laptop and tell the optimizer about the outcome\n5. go to step 1\n\nA setup like this is difficult to implement with the ***_minimize()** function\ninterface. This is why **scikit-optimize** has a ask-and-tell interface that\nyou can use when you want to control the execution of the optimization loop.\n\nThis notebook demonstrates how to use the ask and tell interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\nimport numpy as np\nnp.random.seed(1234)\n\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Setup\n---------\nWe will use a simple 1D problem to illustrate the API. This is a little bit\nartificial as you normally would not use the ask-and-tell interface if you\nhad a function you can call to evaluate the objective.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skopt.learning import ExtraTreesRegressor\nfrom skopt import Optimizer\n\nnoise_level = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our 1D toy problem, this is the function we are trying to\nminimize\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def objective(x, noise_level=noise_level):\n    return np.sin(5 * x[0]) * (1 - np.tanh(x[0] ** 2))\\\n           + np.random.randn() * noise_level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here a quick plot to visualize what the function looks like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot f(x) + contours\nplt.set_cmap(\"viridis\")\nx = np.linspace(-2, 2, 400).reshape(-1, 1)\nfx = np.array([objective(x_i, noise_level=0.0) for x_i in x])\nplt.plot(x, fx, \"r--\", label=\"True (unknown)\")\nplt.fill(np.concatenate([x, x[::-1]]),\n         np.concatenate(([fx_i - 1.9600 * noise_level for fx_i in fx],\n                         [fx_i + 1.9600 * noise_level for fx_i in fx[::-1]])),\n         alpha=.2, fc=\"r\", ec=\"None\")\nplt.legend()\nplt.grid()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we setup the :class:`Optimizer` class. The arguments follow the meaning and\nnaming of the ***_minimize()** functions. An important difference is that\nyou do not pass the objective function to the optimizer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = Optimizer([(-2.0, 2.0)], \"GP\", acq_func=\"EI\",\n                acq_optimizer=\"sampling\",\n                initial_point_generator=\"lhs\")\n\n# To obtain a suggestion for the point at which to evaluate the objective\n# you call the ask() method of opt:\n\nnext_x = opt.ask()\nprint(next_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a real world use case you would probably go away and use this\nparameter in your experiment and come back a while later with the\nresult. In this example we can simply evaluate the objective function\nand report the value back to the optimizer:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f_val = objective(next_x)\nopt.tell(next_x, f_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like ***_minimize()** the first few points are suggestions from\nthe initial point generator as there\nis no data yet with which to fit a surrogate model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in range(9):\n    next_x = opt.ask()\n    f_val = objective(next_x)\n    opt.tell(next_x, f_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot the random suggestions and the first model that has been\nfit:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skopt.acquisition import gaussian_ei\n\n\ndef plot_optimizer(opt, x, fx):\n    model = opt.models[-1]\n    x_model = opt.space.transform(x.tolist())\n\n    # Plot true function.\n    plt.plot(x, fx, \"r--\", label=\"True (unknown)\")\n    plt.fill(np.concatenate([x, x[::-1]]),\n             np.concatenate([fx - 1.9600 * noise_level,\n                             fx[::-1] + 1.9600 * noise_level]),\n             alpha=.2, fc=\"r\", ec=\"None\")\n\n    # Plot Model(x) + contours\n    y_pred, sigma = model.predict(x_model, return_std=True)\n    plt.plot(x, y_pred, \"g--\", label=r\"$\\mu(x)$\")\n    plt.fill(np.concatenate([x, x[::-1]]),\n             np.concatenate([y_pred - 1.9600 * sigma,\n                             (y_pred + 1.9600 * sigma)[::-1]]),\n             alpha=.2, fc=\"g\", ec=\"None\")\n\n    # Plot sampled points\n    plt.plot(opt.Xi, opt.yi,\n             \"r.\", markersize=8, label=\"Observations\")\n\n    acq = gaussian_ei(x_model, model, y_opt=np.min(opt.yi))\n    # shift down to make a better plot\n    acq = 4 * acq - 2\n    plt.plot(x, acq, \"b\", label=\"EI(x)\")\n    plt.fill_between(x.ravel(), -2.0, acq.ravel(), alpha=0.3, color='blue')\n\n    # Adjust plot layout\n    plt.grid()\n    plt.legend(loc='best')\n\n\nplot_optimizer(opt, x, fx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us sample a few more points and plot the optimizer again:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n    next_x = opt.ask()\n    f_val = objective(next_x)\n    opt.tell(next_x, f_val)\n\nplot_optimizer(opt, x, fx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By using the :class:`Optimizer` class directly you get control over the\noptimization loop.\n\nYou can also pickle your :class:`Optimizer` instance if you want to end the\nprocess running it and resume it later. This is handy if your experiment\ntakes a very long time and you want to shutdown your computer in the\nmeantime:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pickle\n\nwith open('my-optimizer.pkl', 'wb') as f:\n    pickle.dump(opt, f)\n\nwith open('my-optimizer.pkl', 'rb') as f:\n    opt_restored = pickle.load(f)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}