{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Partial Dependence Plots  with categorical values\n\nSigurd Carlsen Feb 2019\nHolger Nahrstaedt 2020\n\n.. currentmodule:: skopt\n\nPlot objective now supports optional use of partial dependence as well as\ndifferent methods of defining parameter values for dependency plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\nimport sys\nfrom skopt.plots import plot_objective\nfrom skopt import forest_minimize\nimport numpy as np\nnp.random.seed(123)\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom skopt.space import Integer, Categorical\nfrom skopt import plots, gp_minimize\nfrom skopt.plots import plot_objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## objective function\nHere we define a function that we evaluate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def objective(params):\n    clf = DecisionTreeClassifier(\n        **{dim.name: val for dim, val in\n           zip(SPACE, params) if dim.name != 'dummy'})\n    return -np.mean(cross_val_score(clf, *load_breast_cancer(True)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian optimization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SPACE = [\n    Integer(1, 20, name='max_depth'),\n    Integer(2, 100, name='min_samples_split'),\n    Integer(5, 30, name='min_samples_leaf'),\n    Integer(1, 30, name='max_features'),\n    Categorical(list('abc'), name='dummy'),\n    Categorical(['gini', 'entropy'], name='criterion'),\n    Categorical(list('def'), name='dummy'),\n]\n\nresult = gp_minimize(objective, SPACE, n_calls=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partial dependence plot\n\nHere we see an example of using partial dependence. Even when setting\nn_points all the way down to 10 from the default of 40, this method is\nstill very slow. This is because partial dependence calculates 250 extra\npredictions for each point on the plots.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = plot_objective(result, n_points=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot without partial dependence\nHere we plot without partial dependence. We see that it is a lot faster.\nAlso the values for the other parameters are set to the default \"result\"\nwhich is the parameter set of the best observed value so far. In the case\nof funny_func this is close to 0 for all parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = plot_objective(result,  sample_source='result', n_points=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modify the shown minimum\nHere we try with setting the other parameters to something other than\n\"result\". When dealing with categorical dimensions we can't use\n'expected_minimum'. Therefore we try with \"expected_minimum_random\"\nwhich is a naive way of finding the minimum of the surrogate by only\nusing random sampling. `n_minimum_search` sets the number of random samples,\nwhich is used to find the minimum\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = plot_objective(result, n_points=10, sample_source='expected_minimum_random',\n                   minimum='expected_minimum_random', n_minimum_search=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set a minimum location\nLastly we can also define these parameters ourselfs by\nparsing a list as the pars argument:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = plot_objective(result, n_points=10, sample_source=[15, 4, 7, 15, 'b', 'entropy', 'e'],\n                   minimum=[15, 4, 7, 15, 'b', 'entropy', 'e'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}