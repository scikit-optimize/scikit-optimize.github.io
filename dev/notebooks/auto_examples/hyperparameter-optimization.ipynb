{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n============================================\nTuning a scikit-learn estimator with `skopt`\n============================================\n\nGilles Louppe, July 2016\nKatie Malone, August 2016\nReformatted by Holger Nahrstaedt 2020\n\n.. currentmodule:: skopt\n\nIf you are looking for a :obj:`sklearn.model_selection.GridSearchCV` replacement checkout\n`sphx_glr_auto_examples_sklearn-gridsearchcv-replacement.py` instead.\n\nProblem statement\n=================\n\nTuning the hyper-parameters of a machine learning model is often carried out\nusing an exhaustive exploration of (a subset of) the space all hyper-parameter\nconfigurations (e.g., using :obj:`sklearn.model_selection.GridSearchCV`), which\noften results in a very time consuming operation.\n\nIn this notebook, we illustrate how to couple :class:`gp_minimize` with sklearn's\nestimators to tune hyper-parameters using sequential model-based optimisation,\nhopefully resulting in equivalent or better solutions, but within less\nevaluations.\n\nNote: scikit-optimize provides a dedicated interface for estimator tuning via\n:class:`BayesSearchCV` class which has a similar interface to those of\n:obj:`sklearn.model_selection.GridSearchCV`. This class uses functions of skopt to perform hyperparameter\nsearch efficiently. For example usage of this class, see\n`sphx_glr_auto_examples_sklearn-gridsearchcv-replacement.py`\nexample notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Objective\n=========\nTo tune the hyper-parameters of our model we need to define a model,\ndecide which parameters to optimize, and define the objective function\nwe want to minimize.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_boston\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\n\nboston = load_boston()\nX, y = boston.data, boston.target\nn_features = X.shape[1]\n\n# gradient boosted trees tend to do well on problems like this\nreg = GradientBoostingRegressor(n_estimators=50, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we need to define the bounds of the dimensions of the search space\nwe want to explore and pick the objective. In this case the cross-validation\nmean absolute error of a gradient boosting regressor over the Boston\ndataset, as a function of its hyper-parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\n\n\n# The list of hyper-parameters we want to optimize. For each one we define the\n# bounds, the corresponding scikit-learn parameter name, as well as how to\n# sample values from that dimension (`'log-uniform'` for the learning rate)\nspace  = [Integer(1, 5, name='max_depth'),\n          Real(10**-5, 10**0, \"log-uniform\", name='learning_rate'),\n          Integer(1, n_features, name='max_features'),\n          Integer(2, 100, name='min_samples_split'),\n          Integer(1, 100, name='min_samples_leaf')]\n\n# this decorator allows your objective function to receive a the parameters as\n# keyword arguments. This is particularly convenient when you want to set\n# scikit-learn estimator parameters\n@use_named_args(space)\ndef objective(**params):\n    reg.set_params(**params)\n\n    return -np.mean(cross_val_score(reg, X, y, cv=5, n_jobs=-1,\n                                    scoring=\"neg_mean_absolute_error\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimize all the things!\n========================\nWith these two pieces, we are now ready for sequential model-based\noptimisation. Here we use gaussian process-based optimisation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skopt import gp_minimize\nres_gp = gp_minimize(objective, space, n_calls=50, random_state=0)\n\n\"Best score=%.4f\" % res_gp.fun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\"\"Best parameters:\n- max_depth=%d\n- learning_rate=%.6f\n- max_features=%d\n- min_samples_split=%d\n- min_samples_leaf=%d\"\"\" % (res_gp.x[0], res_gp.x[1],\n                            res_gp.x[2], res_gp.x[3],\n                            res_gp.x[4]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convergence plot\n================\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skopt.plots import plot_convergence\n\nplot_convergence(res_gp)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}