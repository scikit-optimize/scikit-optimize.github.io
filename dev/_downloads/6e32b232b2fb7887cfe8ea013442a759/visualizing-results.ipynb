{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Visualizing optimization results\n\nTim Head, August 2016.\nReformatted by Holger Nahrstaedt 2020\n\n.. currentmodule:: skopt\n\nBayesian optimization or sequential model-based optimization uses a surrogate\nmodel to model the expensive to evaluate objective function `func`. It is\nthis model that is used to determine at which points to evaluate the expensive\nobjective next.\n\nTo help understand why the optimization process is proceeding the way it is,\nit is useful to plot the location and order of the points at which the\nobjective is evaluated. If everything is working as expected, early samples\nwill be spread over the whole parameter space and later samples should\ncluster around the minimum.\n\nThe :class:`plots.plot_evaluations` function helps with visualizing the location and\norder in which samples are evaluated for objectives with an arbitrary\nnumber of dimensions.\n\nThe :class:`plots.plot_objective` function plots the partial dependence of the objective,\nas represented by the surrogate model, for each dimension and as pairs of the\ninput dimensions.\n\nAll of the minimizers implemented in `skopt` return an [`OptimizeResult`]()\ninstance that can be inspected. Both :class:`plots.plot_evaluations` and :class:`plots.plot_objective`\nare helpers that do just that\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\nimport numpy as np\nnp.random.seed(123)\n\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Toy models\n\nWe will use two different toy models to demonstrate how :class:`plots.plot_evaluations`\nworks.\n\nThe first model is the :class:`benchmarks.branin` function which has two dimensions and three\nminima.\n\nThe second model is the `hart6` function which has six dimension which makes\nit hard to visualize. This will show off the utility of\n:class:`plots.plot_evaluations`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skopt.benchmarks import branin as branin\nfrom skopt.benchmarks import hart6 as hart6_\n\n\n# redefined `hart6` to allow adding arbitrary \"noise\" dimensions\ndef hart6(x):\n    return hart6_(x[:6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Starting with `branin`\n\nTo start let's take advantage of the fact that :class:`benchmarks.branin` is a simple\nfunction which can be visualised in two dimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import LogNorm\n\n\ndef plot_branin():\n    fig, ax = plt.subplots()\n\n    x1_values = np.linspace(-5, 10, 100)\n    x2_values = np.linspace(0, 15, 100)\n    x_ax, y_ax = np.meshgrid(x1_values, x2_values)\n    vals = np.c_[x_ax.ravel(), y_ax.ravel()]\n    fx = np.reshape([branin(val) for val in vals], (100, 100))\n\n    cm = ax.pcolormesh(x_ax, y_ax, fx,\n                       norm=LogNorm(vmin=fx.min(),\n                                    vmax=fx.max()),\n                       cmap='viridis_r')\n\n    minima = np.array([[-np.pi, 12.275], [+np.pi, 2.275], [9.42478, 2.475]])\n    ax.plot(minima[:, 0], minima[:, 1], \"r.\", markersize=14,\n            lw=0, label=\"Minima\")\n\n    cb = fig.colorbar(cm)\n    cb.set_label(\"f(x)\")\n\n    ax.legend(loc=\"best\", numpoints=1)\n\n    ax.set_xlabel(\"$X_0$\")\n    ax.set_xlim([-5, 10])\n    ax.set_ylabel(\"$X_1$\")\n    ax.set_ylim([0, 15])\n\n\nplot_branin()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating the objective function\n\nNext we use an extra trees based minimizer to find one of the minima of the\n:class:`benchmarks.branin` function. Then we visualize at which points the objective is being\nevaluated using :class:`plots.plot_evaluations`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\nfrom skopt.plots import plot_evaluations\nfrom skopt import gp_minimize, forest_minimize, dummy_minimize\n\n\nbounds = [(-5.0, 10.0), (0.0, 15.0)]\nn_calls = 160\n\nforest_res = forest_minimize(branin, bounds, n_calls=n_calls,\n                             base_estimator=\"ET\", random_state=4)\n\n_ = plot_evaluations(forest_res, bins=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`plots.plot_evaluations` creates a grid of size `n_dims` by `n_dims`.\nThe diagonal shows histograms for each of the dimensions. In the lower\ntriangle (just one plot in this case) a two dimensional scatter plot of all\npoints is shown. The order in which points were evaluated is encoded in the\ncolor of each point. Darker/purple colors correspond to earlier samples and\nlighter/yellow colors correspond to later samples. A red point shows the\nlocation of the minimum found by the optimization process.\n\nYou should be able to see that points start clustering around the location\nof the true miminum. The histograms show that the objective is evaluated\nmore often at locations near to one of the three minima.\n\nUsing :class:`plots.plot_objective` we can visualise the one dimensional partial\ndependence of the surrogate model for each dimension. The contour plot in\nthe bottom left corner shows the two dimensional partial dependence. In this\ncase this is the same as simply plotting the objective as it only has two\ndimensions.\n\n### Partial dependence plots\n\nPartial dependence plots were proposed by\n[Friedman (2001)]_\nas a method for interpreting the importance of input features used in\ngradient boosting machines. Given a function of $k$: variables\n$y=f\\left(x_1, x_2, ..., x_k\\right)$: the\npartial dependence of $f$ on the $i$-th variable $x_i$ is calculated as:\n$\\phi\\left( x_i \\right) = \\frac{1}{N} \\sum^N_{j=0}f\\left(x_{1,j}, x_{2,j}, ..., x_i, ..., x_{k,j}\\right)$:\nwith the sum running over a set of $N$ points drawn at random from the\nsearch space.\n\nThe idea is to visualize how the value of $x_j$: influences the function\n$f$: after averaging out the influence of all other variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skopt.plots import plot_objective\n\n_ = plot_objective(forest_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The two dimensional partial dependence plot can look like the true\nobjective but it does not have to. As points at which the objective function\nis being evaluated are concentrated around the suspected minimum the\nsurrogate model sometimes is not a good representation of the objective far\naway from the minima.\n\n## Random sampling\n\nCompare this to a minimizer which picks points at random. There is no\nstructure visible in the order in which it evaluates the objective. Because\nthere is no model involved in the process of picking sample points at\nrandom, we can not plot the partial dependence of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dummy_res = dummy_minimize(branin, bounds, n_calls=n_calls, random_state=4)\n\n_ = plot_evaluations(dummy_res, bins=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working in six dimensions\n\nVisualising what happens in two dimensions is easy, where\n:class:`plots.plot_evaluations` and :class:`plots.plot_objective` start to be useful is when the\nnumber of dimensions grows. They take care of many of the more mundane\nthings needed to make good plots of all combinations of the dimensions.\n\nThe next example uses class:`benchmarks.hart6` which has six dimensions and shows both\n:class:`plots.plot_evaluations` and :class:`plots.plot_objective`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bounds = [(0., 1.),] * 6\n\nforest_res = forest_minimize(hart6, bounds, n_calls=n_calls,\n                             base_estimator=\"ET\", random_state=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_ = plot_evaluations(forest_res)\n_ = plot_objective(forest_res, n_samples=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Going from 6 to 6+2 dimensions\n\nTo make things more interesting let's add two dimension to the problem.\nAs :class:`benchmarks.hart6` only depends on six dimensions we know that for this problem\nthe new dimensions will be \"flat\" or uninformative. This is clearly visible\nin both the placement of samples and the partial dependence plots.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bounds = [(0., 1.),] * 8\nn_calls = 200\n\nforest_res = forest_minimize(hart6, bounds, n_calls=n_calls,\n                             base_estimator=\"ET\", random_state=4)\n\n_ = plot_evaluations(forest_res)\n_ = plot_objective(forest_res, n_samples=40)\n\n# .. [Friedman (2001)] `doi:10.1214/aos/1013203451 section 8.2 <http://projecteuclid.org/euclid.aos/1013203451>`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}