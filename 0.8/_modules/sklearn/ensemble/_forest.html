

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="Description" content="scikit-optimize: machine learning in Python">

  
  <title>sklearn.ensemble._forest &mdash; scikit-optimize 0.8.0 documentation</title>
  
  <link rel="canonical" href="https://scikit-optimize.github.io/_modules/sklearn/ensemble/_forest.html" />

  
  <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../../../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script> 
</head>
<body>
<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../../../index.html">
        <img
          class="sk-brand-img"
          src="../../../_static/logo.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../../install.html">Install</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../../user_guide.html">User Guide</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../../modules/classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../../auto_examples/index.html">Examples</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../../getting_started.html">Getting Started</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../../development.html">Development</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://github.com/scikit-optimize/scikit-optimize">GitHub</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-optimize.github.io/dev/versions.html">Other Versions</a>
        </li>
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="sk-nav-dropdown-item dropdown-item" href="../../../getting_started.html">Getting Started</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../../development.html">Development</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://github.com/scikit-optimize/scikit-optimize">GitHub</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-optimize.github.io/dev/versions.html">Other Versions</a>
          </div>
        </li>
      </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../../../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Go" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Toggle Menu</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
          <a href="../../../index.html">
            <img
              class="sk-brand-img"
              src="../../../_static/logo.png"
              alt="logo"/>
          </a>
        </div>
        <div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
            <a href="#" role="button" class="btn sk-btn-rellink py-1 disabled"">Prev</a><a href="../../index.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="Module code">Up</a>
            <a href="#" role="button" class="btn sk-btn-rellink py-1 disabled"">Next</a>
        </div>
        <div class="alert alert-danger p-1 mb-2" role="alert">
          <p class="text-center mb-0">
          <strong>scikit-optimize 0.8.0</strong><br/>
            <a href="https://scikit-optimize.github.io/dev/versions.html">Other versions</a>
          </p>
        </div>
          <div class="sk-sidebar-toc">
            
          </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <h1>Source code for sklearn.ensemble._forest</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Forest of trees-based ensemble methods.</span>

<span class="sd">Those methods include random forests and extremely randomized trees.</span>

<span class="sd">The module structure is the following:</span>

<span class="sd">- The ``BaseForest`` base class implements a common ``fit`` method for all</span>
<span class="sd">  the estimators in the module. The ``fit`` method of the base ``Forest``</span>
<span class="sd">  class calls the ``fit`` method of each sub-estimator on random samples</span>
<span class="sd">  (with replacement, a.k.a. bootstrap) of the training set.</span>

<span class="sd">  The init of the sub-estimator is further delegated to the</span>
<span class="sd">  ``BaseEnsemble`` constructor.</span>

<span class="sd">- The ``ForestClassifier`` and ``ForestRegressor`` base classes further</span>
<span class="sd">  implement the prediction logic by computing an average of the predicted</span>
<span class="sd">  outcomes of the sub-estimators.</span>

<span class="sd">- The ``RandomForestClassifier`` and ``RandomForestRegressor`` derived</span>
<span class="sd">  classes provide the user with concrete implementations of</span>
<span class="sd">  the forest ensemble method using classical, deterministic</span>
<span class="sd">  ``DecisionTreeClassifier`` and ``DecisionTreeRegressor`` as</span>
<span class="sd">  sub-estimator implementations.</span>

<span class="sd">- The ``ExtraTreesClassifier`` and ``ExtraTreesRegressor`` derived</span>
<span class="sd">  classes provide the user with concrete implementations of the</span>
<span class="sd">  forest ensemble method using the extremely randomized trees</span>
<span class="sd">  ``ExtraTreeClassifier`` and ``ExtraTreeRegressor`` as</span>
<span class="sd">  sub-estimator implementations.</span>

<span class="sd">Single and multi-output problems are both handled.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="c1"># Authors: Gilles Louppe &lt;g.louppe@gmail.com&gt;</span>
<span class="c1">#          Brian Holt &lt;bdholt1@gmail.com&gt;</span>
<span class="c1">#          Joly Arnaud &lt;arnaud.v.joly@gmail.com&gt;</span>
<span class="c1">#          Fares Hedayati &lt;fares.hedayati@gmail.com&gt;</span>
<span class="c1">#</span>
<span class="c1"># License: BSD 3 clause</span>


<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">from</span> <span class="nn">warnings</span> <span class="kn">import</span> <span class="n">catch_warnings</span><span class="p">,</span> <span class="n">simplefilter</span><span class="p">,</span> <span class="n">warn</span>
<span class="kn">import</span> <span class="nn">threading</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">issparse</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">hstack</span> <span class="k">as</span> <span class="n">sparse_hstack</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>

<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">MultiOutputMixin</span>
<span class="kn">from</span> <span class="nn">..metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">..preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">..tree</span> <span class="kn">import</span> <span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">DecisionTreeRegressor</span><span class="p">,</span>
                    <span class="n">ExtraTreeClassifier</span><span class="p">,</span> <span class="n">ExtraTreeRegressor</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">..tree._tree</span> <span class="kn">import</span> <span class="n">DTYPE</span><span class="p">,</span> <span class="n">DOUBLE</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">check_random_state</span><span class="p">,</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">compute_sample_weight</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="kn">import</span> <span class="n">DataConversionWarning</span>
<span class="kn">from</span> <span class="nn">._base</span> <span class="kn">import</span> <span class="n">BaseEnsemble</span><span class="p">,</span> <span class="n">_partition_estimators</span>
<span class="kn">from</span> <span class="nn">..utils.fixes</span> <span class="kn">import</span> <span class="n">_joblib_parallel_args</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="kn">import</span> <span class="n">check_classification_targets</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="n">check_is_fitted</span><span class="p">,</span> <span class="n">_check_sample_weight</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="n">_deprecate_positional_args</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;RandomForestClassifier&quot;</span><span class="p">,</span>
           <span class="s2">&quot;RandomForestRegressor&quot;</span><span class="p">,</span>
           <span class="s2">&quot;ExtraTreesClassifier&quot;</span><span class="p">,</span>
           <span class="s2">&quot;ExtraTreesRegressor&quot;</span><span class="p">,</span>
           <span class="s2">&quot;RandomTreesEmbedding&quot;</span><span class="p">]</span>

<span class="n">MAX_INT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>


<span class="k">def</span> <span class="nf">_get_n_samples_bootstrap</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">max_samples</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the number of samples in a bootstrap sample.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples : int</span>
<span class="sd">        Number of samples in the dataset.</span>
<span class="sd">    max_samples : int or float</span>
<span class="sd">        The maximum number of samples to draw from the total available:</span>
<span class="sd">            - if float, this indicates a fraction of the total and should be</span>
<span class="sd">              the interval `(0, 1)`;</span>
<span class="sd">            - if int, this indicates the exact number of samples;</span>
<span class="sd">            - if None, this indicates the total number of samples.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    n_samples_bootstrap : int</span>
<span class="sd">        The total number of samples to draw for the bootstrap sample.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">max_samples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">n_samples</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_samples</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">max_samples</span> <span class="o">&lt;=</span> <span class="n">n_samples</span><span class="p">):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;`max_samples` must be in range 1 to </span><span class="si">{}</span><span class="s2"> but got value </span><span class="si">{}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">max_samples</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">max_samples</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_samples</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Real</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;</span> <span class="n">max_samples</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;`max_samples` must be in range (0, 1) but got value </span><span class="si">{}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_samples</span><span class="p">))</span>
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">max_samples</span><span class="p">))</span>

    <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;`max_samples` should be int or float, but got type &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">max_samples</span><span class="p">)))</span>


<span class="k">def</span> <span class="nf">_generate_sample_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples_bootstrap</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Private function used to _parallel_build_trees function.&quot;&quot;&quot;</span>

    <span class="n">random_instance</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">random_instance</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples_bootstrap</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sample_indices</span>


<span class="k">def</span> <span class="nf">_generate_unsampled_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples_bootstrap</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Private function used to forest._set_oob_score function.&quot;&quot;&quot;</span>
    <span class="n">sample_indices</span> <span class="o">=</span> <span class="n">_generate_sample_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span>
                                              <span class="n">n_samples_bootstrap</span><span class="p">)</span>
    <span class="n">sample_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">sample_indices</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">unsampled_mask</span> <span class="o">=</span> <span class="n">sample_counts</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">indices_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">unsampled_indices</span> <span class="o">=</span> <span class="n">indices_range</span><span class="p">[</span><span class="n">unsampled_mask</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">unsampled_indices</span>


<span class="k">def</span> <span class="nf">_parallel_build_trees</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">forest</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">tree_idx</span><span class="p">,</span> <span class="n">n_trees</span><span class="p">,</span>
                          <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">n_samples_bootstrap</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Private function used to fit a single tree in parallel.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;building tree </span><span class="si">%d</span><span class="s2"> of </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tree_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_trees</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">forest</span><span class="o">.</span><span class="n">bootstrap</span><span class="p">:</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">curr_sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">curr_sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="n">indices</span> <span class="o">=</span> <span class="n">_generate_sample_indices</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span>
                                           <span class="n">n_samples_bootstrap</span><span class="p">)</span>
        <span class="n">sample_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
        <span class="n">curr_sample_weight</span> <span class="o">*=</span> <span class="n">sample_counts</span>

        <span class="k">if</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s1">&#39;subsample&#39;</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">catch_warnings</span><span class="p">():</span>
                <span class="n">simplefilter</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
                <span class="n">curr_sample_weight</span> <span class="o">*=</span> <span class="n">compute_sample_weight</span><span class="p">(</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                                            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">class_weight</span> <span class="o">==</span> <span class="s1">&#39;balanced_subsample&#39;</span><span class="p">:</span>
            <span class="n">curr_sample_weight</span> <span class="o">*=</span> <span class="n">compute_sample_weight</span><span class="p">(</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                                        <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">)</span>

        <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">curr_sample_weight</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tree</span>


<span class="k">class</span> <span class="nc">BaseForest</span><span class="p">(</span><span class="n">MultiOutputMixin</span><span class="p">,</span> <span class="n">BaseEnsemble</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for forests of trees.</span>

<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_estimator</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
                 <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(),</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span> <span class="o">=</span> <span class="n">bootstrap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span> <span class="o">=</span> <span class="n">oob_score</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_samples</span> <span class="o">=</span> <span class="n">max_samples</span>

    <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply trees in the forest to X, return leaf indices.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_leaves : ndarray of shape (n_samples, n_estimators)</span>
<span class="sd">            For each datapoint x in X and for each tree in the forest,</span>
<span class="sd">            return the index of the leaf x ends up in.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                           <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">prefer</span><span class="o">=</span><span class="s2">&quot;threads&quot;</span><span class="p">))(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">apply</span><span class="p">)(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">decision_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the decision path in the forest.</span>

<span class="sd">        .. versionadded:: 0.18</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        indicator : sparse matrix of shape (n_samples, n_nodes)</span>
<span class="sd">            Return a node indicator matrix where non zero elements indicates</span>
<span class="sd">            that the samples goes through the nodes. The matrix is of CSR</span>
<span class="sd">            format.</span>

<span class="sd">        n_nodes_ptr : ndarray of shape (n_estimators + 1,)</span>
<span class="sd">            The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]</span>
<span class="sd">            gives the indicator value for the i-th estimator.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">indicators</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                              <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">prefer</span><span class="o">=</span><span class="s1">&#39;threads&#39;</span><span class="p">))(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">decision_path</span><span class="p">)(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="n">n_nodes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indicators</span><span class="p">])</span>
        <span class="n">n_nodes_ptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">)</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">sparse_hstack</span><span class="p">(</span><span class="n">indicators</span><span class="p">)</span><span class="o">.</span><span class="n">tocsr</span><span class="p">(),</span> <span class="n">n_nodes_ptr</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build a forest of trees from the training set (X, y).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The training input samples. Internally, its dtype will be converted</span>
<span class="sd">            to ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csc_matrix``.</span>

<span class="sd">        y : array-like of shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">            The target values (class labels in classification, real numbers in</span>
<span class="sd">            regression).</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. In the case of</span>
<span class="sd">            classification, splits are also ignored if they would result in any</span>
<span class="sd">            single class carrying a negative weight in either child node.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Validate or convert input data</span>
        <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;sparse multilabel-indicator for y is not supported.&quot;</span>
            <span class="p">)</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">multi_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">_check_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># Pre-sort indices to avoid that each individual tree of the</span>
            <span class="c1"># ensemble sorts the indices.</span>
            <span class="n">X</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>

        <span class="c1"># Remap output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;A column-vector y was passed when a 1d array was&quot;</span>
                 <span class="s2">&quot; expected. Please change the shape of y to &quot;</span>
                 <span class="s2">&quot;(n_samples,), for example using ravel().&quot;</span><span class="p">,</span>
                 <span class="n">DataConversionWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># reshape is necessary to preserve the data contiguity against vs</span>
            <span class="c1"># [:, np.newaxis] that does not.</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">y</span><span class="p">,</span> <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y_class_weight</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="o">!=</span> <span class="n">DOUBLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">y</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">contiguous</span><span class="p">:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DOUBLE</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">expanded_class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">expanded_class_weight</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expanded_class_weight</span>

        <span class="c1"># Get bootstrap sample size</span>
        <span class="n">n_samples_bootstrap</span> <span class="o">=</span> <span class="n">_get_n_samples_bootstrap</span><span class="p">(</span>
            <span class="n">n_samples</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_samples</span>
        <span class="p">)</span>

        <span class="c1"># Check parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_estimator</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Out of bag estimation only available&quot;</span>
                             <span class="s2">&quot; if bootstrap=True&quot;</span><span class="p">)</span>

        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;estimators_&quot;</span><span class="p">):</span>
            <span class="c1"># Free allocated memory, if any</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">n_more_estimators</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_more_estimators</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;n_estimators=</span><span class="si">%d</span><span class="s1"> must be larger or equal to &#39;</span>
                             <span class="s1">&#39;len(estimators_)=</span><span class="si">%d</span><span class="s1"> when warm_start==True&#39;</span>
                             <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)))</span>

        <span class="k">elif</span> <span class="n">n_more_estimators</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Warm-start fitting without increasing n_estimators does not &quot;</span>
                 <span class="s2">&quot;fit new trees.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># We draw from the random state to get the random state we</span>
                <span class="c1"># would have got if we hadn&#39;t used a warm_start.</span>
                <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">))</span>

            <span class="n">trees</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">append</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                          <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_more_estimators</span><span class="p">)]</span>

            <span class="c1"># Parallel loop: we prefer the threading backend as the Cython code</span>
            <span class="c1"># for fitting the trees is internally releasing the Python GIL</span>
            <span class="c1"># making threading more efficient than multiprocessing in</span>
            <span class="c1"># that case. However, for joblib 0.12+ we respect any</span>
            <span class="c1"># parallel_backend contexts set at a higher level,</span>
            <span class="c1"># since correctness does not rely on using threads.</span>
            <span class="n">trees</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                             <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">prefer</span><span class="o">=</span><span class="s1">&#39;threads&#39;</span><span class="p">))(</span>
                <span class="n">delayed</span><span class="p">(</span><span class="n">_parallel_build_trees</span><span class="p">)(</span>
                    <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">trees</span><span class="p">),</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span>
                    <span class="n">n_samples_bootstrap</span><span class="o">=</span><span class="n">n_samples_bootstrap</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trees</span><span class="p">))</span>

            <span class="c1"># Collect newly grown trees</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">trees</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_oob_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># Decapsulate classes_ attributes</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;classes_&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_set_oob_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate out of bag predictions and score.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_validate_y_class_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Default implementation</span>
        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_validate_X_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Validate X whenever one tries to predict, apply, predict_proba.&quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">feature_importances_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The impurity-based feature importances.</span>

<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>

<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">            The values of this array sum to 1, unless all trees are single node</span>
<span class="sd">            trees consisting of only the root node, in which case it will be an</span>
<span class="sd">            array of zeros.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">all_importances</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span>
                                   <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">prefer</span><span class="o">=</span><span class="s1">&#39;threads&#39;</span><span class="p">))(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="nb">getattr</span><span class="p">)(</span><span class="n">tree</span><span class="p">,</span> <span class="s1">&#39;feature_importances_&#39;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="k">if</span> <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">node_count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">all_importances</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="n">all_importances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_importances</span><span class="p">,</span>
                                  <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_importances</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">all_importances</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_accumulate_prediction</span><span class="p">(</span><span class="n">predict</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">lock</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is a utility function for joblib&#39;s Parallel.</span>

<span class="sd">    It can&#39;t go locally in ForestClassifier or ForestRegressor, because joblib</span>
<span class="sd">    complains that it cannot pickle it when placed there.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">lock</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">prediction</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)):</span>
                <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">prediction</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">ForestClassifier</span><span class="p">(</span><span class="n">ClassifierMixin</span><span class="p">,</span> <span class="n">BaseForest</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for forest of trees-based classifiers.</span>

<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_estimator</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
                 <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(),</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">,</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_oob_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute out-of-bag score.&quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>

        <span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">oob_decision_function</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">oob_score</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_classes_</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
                       <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">)]</span>

        <span class="n">n_samples_bootstrap</span> <span class="o">=</span> <span class="n">_get_n_samples_bootstrap</span><span class="p">(</span>
            <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_samples</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="n">unsampled_indices</span> <span class="o">=</span> <span class="n">_generate_unsampled_indices</span><span class="p">(</span>
                <span class="n">estimator</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples_bootstrap</span><span class="p">)</span>
            <span class="n">p_estimator</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:],</span>
                                                  <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">p_estimator</span> <span class="o">=</span> <span class="p">[</span><span class="n">p_estimator</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
                <span class="n">predictions</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">p_estimator</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Some inputs do not have OOB scores. &quot;</span>
                     <span class="s2">&quot;This probably means too few trees were used &quot;</span>
                     <span class="s2">&quot;to compute any reliable oob estimates.&quot;</span><span class="p">)</span>

            <span class="n">decision</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">/</span>
                        <span class="n">predictions</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
            <span class="n">oob_decision_function</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">decision</span><span class="p">)</span>
            <span class="n">oob_score</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">==</span>
                                 <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_decision_function_</span> <span class="o">=</span> <span class="n">oob_decision_function</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_decision_function_</span> <span class="o">=</span> <span class="n">oob_decision_function</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">=</span> <span class="n">oob_score</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span>

    <span class="k">def</span> <span class="nf">_validate_y_class_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">y_original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">y_store_unique_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="n">classes_k</span><span class="p">,</span> <span class="n">y_store_unique_indices</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span> <span class="n">k</span><span class="p">],</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classes_k</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classes_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y_store_unique_indices</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">valid_presets</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span> <span class="s1">&#39;balanced_subsample&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_presets</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Valid presets for class_weight include &#39;</span>
                                     <span class="s1">&#39;&quot;balanced&quot; and &quot;balanced_subsample&quot;.&#39;</span>
                                     <span class="s1">&#39;Given &quot;</span><span class="si">%s</span><span class="s1">&quot;.&#39;</span>
                                     <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
                    <span class="n">warn</span><span class="p">(</span><span class="s1">&#39;class_weight presets &quot;balanced&quot; or &#39;</span>
                         <span class="s1">&#39;&quot;balanced_subsample&quot; are &#39;</span>
                         <span class="s1">&#39;not recommended for warm_start if the fitted data &#39;</span>
                         <span class="s1">&#39;differs from the full dataset. In order to use &#39;</span>
                         <span class="s1">&#39;&quot;balanced&quot; weights, use compute_class_weight &#39;</span>
                         <span class="s1">&#39;(&quot;balanced&quot;, classes, y). In place of y you can use &#39;</span>
                         <span class="s1">&#39;a large enough sample of the full training set &#39;</span>
                         <span class="s1">&#39;target to properly estimate the class frequency &#39;</span>
                         <span class="s1">&#39;distributions. Pass the resulting weights as the &#39;</span>
                         <span class="s1">&#39;class_weight parameter.&#39;</span><span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">!=</span> <span class="s1">&#39;balanced_subsample&#39;</span> <span class="ow">or</span>
                    <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">==</span> <span class="s2">&quot;balanced_subsample&quot;</span><span class="p">:</span>
                    <span class="n">class_weight</span> <span class="o">=</span> <span class="s2">&quot;balanced&quot;</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span>
                <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="n">compute_sample_weight</span><span class="p">(</span><span class="n">class_weight</span><span class="p">,</span>
                                                              <span class="n">y_original</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">expanded_class_weight</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict class for X.</span>

<span class="sd">        The predicted class of an input sample is a vote by the trees in</span>
<span class="sd">        the forest, weighted by their probability estimates. That is,</span>
<span class="sd">        the predicted class is the one with highest mean probability</span>
<span class="sd">        estimate across the trees.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">            The predicted classes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proba</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">n_samples</span> <span class="o">=</span> <span class="n">proba</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># all dtypes should be the same, so just take the first</span>
            <span class="n">class_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">),</span>
                                   <span class="n">dtype</span><span class="o">=</span><span class="n">class_type</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
                <span class="n">predictions</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">proba</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                                                                    <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                                                          <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">predictions</span>

    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict class probabilities for X.</span>

<span class="sd">        The predicted class probabilities of an input sample are computed as</span>
<span class="sd">        the mean predicted class probabilities of the trees in the forest.</span>
<span class="sd">        The class probability of a single tree is the fraction of samples of</span>
<span class="sd">        the same class in a leaf.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : ndarray of shape (n_samples, n_classes), or a list of n_outputs</span>
<span class="sd">            such arrays if n_outputs &gt; 1.</span>
<span class="sd">            The class probabilities of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># Check data</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Assign chunk of trees to jobs</span>
        <span class="n">n_jobs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_partition_estimators</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span>

        <span class="c1"># avoid storing the output of every estimator by summing them here</span>
        <span class="n">all_proba</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">j</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
                     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">)]</span>
        <span class="n">lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
        <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">require</span><span class="o">=</span><span class="s2">&quot;sharedmem&quot;</span><span class="p">))(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_accumulate_prediction</span><span class="p">)(</span><span class="n">e</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">all_proba</span><span class="p">,</span>
                                            <span class="n">lock</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">proba</span> <span class="ow">in</span> <span class="n">all_proba</span><span class="p">:</span>
            <span class="n">proba</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_proba</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">all_proba</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">all_proba</span>

    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict class log-probabilities for X.</span>

<span class="sd">        The predicted class log-probabilities of an input sample is computed as</span>
<span class="sd">        the log of the mean predicted class probabilities of the trees in the</span>
<span class="sd">        forest.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        p : ndarray of shape (n_samples, n_classes), or a list of n_outputs</span>
<span class="sd">            such arrays if n_outputs &gt; 1.</span>
<span class="sd">            The class probabilities of the input samples. The order of the</span>
<span class="sd">            classes corresponds to that in the attribute :term:`classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">proba</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
                <span class="n">proba</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">proba</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>

            <span class="k">return</span> <span class="n">proba</span>


<span class="k">class</span> <span class="nc">ForestRegressor</span><span class="p">(</span><span class="n">RegressorMixin</span><span class="p">,</span> <span class="n">BaseForest</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for forest of trees-based regressors.</span>

<span class="sd">    Warning: This class should not be used directly. Use derived classes</span>
<span class="sd">    instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">base_estimator</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
                 <span class="n">estimator_params</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(),</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="p">,</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="n">estimator_params</span><span class="p">,</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict regression target for X.</span>

<span class="sd">        The predicted regression target of an input sample is computed as the</span>
<span class="sd">        mean predicted regression targets of the trees in the forest.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Internally, its dtype will be converted to</span>
<span class="sd">            ``dtype=np.float32``. If a sparse matrix is provided, it will be</span>
<span class="sd">            converted into a sparse ``csr_matrix``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)</span>
<span class="sd">            The predicted values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># Check data</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Assign chunk of trees to jobs</span>
        <span class="n">n_jobs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">_partition_estimators</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)</span>

        <span class="c1"># avoid storing the output of every estimator by summing them here</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

        <span class="c1"># Parallel loop</span>
        <span class="n">lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
        <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">require</span><span class="o">=</span><span class="s2">&quot;sharedmem&quot;</span><span class="p">))(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_accumulate_prediction</span><span class="p">)(</span><span class="n">e</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="n">y_hat</span><span class="p">],</span> <span class="n">lock</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="n">y_hat</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_hat</span>

    <span class="k">def</span> <span class="nf">_set_oob_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute out-of-bag scores.&quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>

        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">))</span>
        <span class="n">n_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">))</span>

        <span class="n">n_samples_bootstrap</span> <span class="o">=</span> <span class="n">_get_n_samples_bootstrap</span><span class="p">(</span>
            <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_samples</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="n">unsampled_indices</span> <span class="o">=</span> <span class="n">_generate_unsampled_indices</span><span class="p">(</span>
                <span class="n">estimator</span><span class="o">.</span><span class="n">random_state</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples_bootstrap</span><span class="p">)</span>
            <span class="n">p_estimator</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
                <span class="n">X</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:],</span> <span class="n">check_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">p_estimator</span> <span class="o">=</span> <span class="n">p_estimator</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

            <span class="n">predictions</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">p_estimator</span>
            <span class="n">n_predictions</span><span class="p">[</span><span class="n">unsampled_indices</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">n_predictions</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Some inputs do not have OOB scores. &quot;</span>
                 <span class="s2">&quot;This probably means too few trees were used &quot;</span>
                 <span class="s2">&quot;to compute any reliable oob estimates.&quot;</span><span class="p">)</span>
            <span class="n">n_predictions</span><span class="p">[</span><span class="n">n_predictions</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">predictions</span> <span class="o">/=</span> <span class="n">n_predictions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oob_prediction_</span> <span class="o">=</span> <span class="n">predictions</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_prediction_</span> <span class="o">=</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">oob_prediction_</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">=</span> <span class="mf">0.0</span>

        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">+=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span> <span class="n">k</span><span class="p">],</span>
                                        <span class="n">predictions</span><span class="p">[:,</span> <span class="n">k</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">oob_score_</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span>

    <span class="k">def</span> <span class="nf">_compute_partial_dependence_recursion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">target_features</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fast partial dependence computation.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        grid : ndarray of shape (n_samples, n_target_features)</span>
<span class="sd">            The grid points on which the partial dependence should be</span>
<span class="sd">            evaluated.</span>
<span class="sd">        target_features : ndarray of shape (n_target_features)</span>
<span class="sd">            The set of target features for which the partial dependence</span>
<span class="sd">            should be evaluated.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        averaged_predictions : ndarray of shape (n_samples,)</span>
<span class="sd">            The value of the partial dependence function on each grid point.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
        <span class="n">averaged_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">grid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                        <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="c1"># Note: we don&#39;t sum in parallel because the GIL isn&#39;t released in</span>
            <span class="c1"># the fast method.</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">compute_partial_dependence</span><span class="p">(</span>
                <span class="n">grid</span><span class="p">,</span> <span class="n">target_features</span><span class="p">,</span> <span class="n">averaged_predictions</span><span class="p">)</span>
        <span class="c1"># Average over the forest</span>
        <span class="n">averaged_predictions</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">averaged_predictions</span>

<span class="k">class</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">ForestClassifier</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A random forest classifier.</span>

<span class="sd">    A random forest is a meta estimator that fits a number of decision tree</span>
<span class="sd">    classifiers on various sub-samples of the dataset and uses averaging to</span>
<span class="sd">    improve the predictive accuracy and control over-fitting.</span>
<span class="sd">    The sub-sample size is controlled with the `max_samples` parameter if</span>
<span class="sd">    `bootstrap=True` (default), otherwise the whole dataset is used to build</span>
<span class="sd">    each tree.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;forest&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : int, default=100</span>
<span class="sd">        The number of trees in the forest.</span>

<span class="sd">        .. versionchanged:: 0.22</span>
<span class="sd">           The default value of ``n_estimators`` changed from 10 to 100</span>
<span class="sd">           in 0.22.</span>

<span class="sd">    criterion : {&quot;gini&quot;, &quot;entropy&quot;}, default=&quot;gini&quot;</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria are</span>
<span class="sd">        &quot;gini&quot; for the Gini impurity and &quot;entropy&quot; for the information gain.</span>
<span class="sd">        Note: this parameter is tree-specific.</span>

<span class="sd">    max_depth : int, default=None</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : {&quot;auto&quot;, &quot;sqrt&quot;, &quot;log2&quot;}, int or float, default=&quot;auto&quot;</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)` (same as &quot;auto&quot;).</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float, default=None</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19. The default value of</span>
<span class="sd">           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it</span>
<span class="sd">           will be removed in 0.25. Use ``min_impurity_decrease`` instead.</span>


<span class="sd">    bootstrap : bool, default=True</span>
<span class="sd">        Whether bootstrap samples are used when building trees. If False, the</span>
<span class="sd">        whole dataset is used to build each tree.</span>

<span class="sd">    oob_score : bool, default=False</span>
<span class="sd">        Whether to use out-of-bag samples to estimate</span>
<span class="sd">        the generalization accuracy.</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,</span>
<span class="sd">        :meth:`decision_path` and :meth:`apply` are all parallelized over the</span>
<span class="sd">        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`</span>
<span class="sd">        context. ``-1`` means using all processors. See :term:`Glossary</span>
<span class="sd">        &lt;n_jobs&gt;` for more details.</span>

<span class="sd">    random_state : int or RandomState, default=None</span>
<span class="sd">        Controls both the randomness of the bootstrapping of the samples used</span>
<span class="sd">        when building trees (if ``bootstrap=True``) and the sampling of the</span>
<span class="sd">        features to consider when looking for the best split at each node</span>
<span class="sd">        (if ``max_features &lt; n_features``).</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;` for details.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    class_weight : {&quot;balanced&quot;, &quot;balanced_subsample&quot;}, dict or list of dicts, \</span>
<span class="sd">            default=None</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one. For</span>
<span class="sd">        multi-output problems, a list of dicts can be provided in the same</span>
<span class="sd">        order as the columns of y.</span>

<span class="sd">        Note that for multioutput (including multilabel) weights should be</span>
<span class="sd">        defined for each class of every column in its own dict. For example,</span>
<span class="sd">        for four-class multilabel classification weights should be</span>
<span class="sd">        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of</span>
<span class="sd">        [{1:1}, {2:5}, {3:1}, {4:1}].</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">        The &quot;balanced_subsample&quot; mode is the same as &quot;balanced&quot; except that</span>
<span class="sd">        weights are computed based on the bootstrap sample for every tree</span>
<span class="sd">        grown.</span>

<span class="sd">        For multi-output, the weights of each column of y will be multiplied.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">        Complexity parameter used for Minimal Cost-Complexity Pruning. The</span>
<span class="sd">        subtree with the largest cost complexity that is smaller than</span>
<span class="sd">        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See</span>
<span class="sd">        :ref:`minimal_cost_complexity_pruning` for details.</span>

<span class="sd">        .. versionadded:: 0.22</span>

<span class="sd">    max_samples : int or float, default=None</span>
<span class="sd">        If bootstrap is True, the number of samples to draw from X</span>
<span class="sd">        to train each base estimator.</span>

<span class="sd">        - If None (default), then draw `X.shape[0]` samples.</span>
<span class="sd">        - If int, then draw `max_samples` samples.</span>
<span class="sd">        - If float, then draw `max_samples * X.shape[0]` samples. Thus,</span>
<span class="sd">          `max_samples` should be in the interval `(0, 1)`.</span>

<span class="sd">        .. versionadded:: 0.22</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator_ : DecisionTreeClassifier</span>
<span class="sd">        The child estimator template used to create the collection of fitted</span>
<span class="sd">        sub-estimators.</span>

<span class="sd">    estimators_ : list of DecisionTreeClassifier</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    classes_ : ndarray of shape (n_classes,) or a list of such arrays</span>
<span class="sd">        The classes labels (single output problem), or a list of arrays of</span>
<span class="sd">        class labels (multi-output problem).</span>

<span class="sd">    n_classes_ : int or list</span>
<span class="sd">        The number of classes (single output problem), or a list containing the</span>
<span class="sd">        number of classes for each output (multi-output problem).</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">        The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>

<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>
<span class="sd">        This attribute exists only when ``oob_score`` is True.</span>

<span class="sd">    oob_decision_function_ : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        Decision function computed with out-of-bag estimate on the training</span>
<span class="sd">        set. If n_estimators is small it might be possible that a data point</span>
<span class="sd">        was never left out during the bootstrap. In this case,</span>
<span class="sd">        `oob_decision_function_` might contain NaN. This attribute exists</span>
<span class="sd">        only when ``oob_score`` is True.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    DecisionTreeClassifier, ExtraTreesClassifier</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The default values for the parameters controlling the size of the trees</span>
<span class="sd">    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and</span>
<span class="sd">    unpruned trees which can potentially be very large on some data sets. To</span>
<span class="sd">    reduce memory consumption, the complexity and size of the trees should be</span>
<span class="sd">    controlled by setting those parameter values.</span>

<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data,</span>
<span class="sd">    ``max_features=n_features`` and ``bootstrap=False``, if the improvement</span>
<span class="sd">    of the criterion is identical for several splits enumerated during the</span>
<span class="sd">    search of the best split. To obtain a deterministic behaviour during</span>
<span class="sd">    fitting, ``random_state`` has to be fixed.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] L. Breiman, &quot;Random Forests&quot;, Machine Learning, 45(1), 5-32, 2001.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_classification</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_classification(n_samples=1000, n_features=4,</span>
<span class="sd">    ...                            n_informative=2, n_redundant=0,</span>
<span class="sd">    ...                            random_state=0, shuffle=False)</span>
<span class="sd">    &gt;&gt;&gt; clf = RandomForestClassifier(max_depth=2, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y)</span>
<span class="sd">    RandomForestClassifier(...)</span>
<span class="sd">    &gt;&gt;&gt; print(clf.predict([[0, 0, 0, 0]]))</span>
<span class="sd">    [1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;gini&quot;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(),</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span> <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;random_state&quot;</span><span class="p">,</span> <span class="s2">&quot;ccp_alpha&quot;</span><span class="p">),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ccp_alpha</span> <span class="o">=</span> <span class="n">ccp_alpha</span>


<span class="k">class</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="n">ForestRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A random forest regressor.</span>

<span class="sd">    A random forest is a meta estimator that fits a number of classifying</span>
<span class="sd">    decision trees on various sub-samples of the dataset and uses averaging</span>
<span class="sd">    to improve the predictive accuracy and control over-fitting.</span>
<span class="sd">    The sub-sample size is controlled with the `max_samples` parameter if</span>
<span class="sd">    `bootstrap=True` (default), otherwise the whole dataset is used to build</span>
<span class="sd">    each tree.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;forest&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : int, default=100</span>
<span class="sd">        The number of trees in the forest.</span>

<span class="sd">        .. versionchanged:: 0.22</span>
<span class="sd">           The default value of ``n_estimators`` changed from 10 to 100</span>
<span class="sd">           in 0.22.</span>

<span class="sd">    criterion : {&quot;mse&quot;, &quot;mae&quot;}, default=&quot;mse&quot;</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;mse&quot; for the mean squared error, which is equal to variance</span>
<span class="sd">        reduction as feature selection criterion, and &quot;mae&quot; for the mean</span>
<span class="sd">        absolute error.</span>

<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">           Mean Absolute Error (MAE) criterion.</span>

<span class="sd">    max_depth : int, default=None</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : {&quot;auto&quot;, &quot;sqrt&quot;, &quot;log2&quot;}, int or float, default=&quot;auto&quot;</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=n_features`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float, default=None</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19. The default value of</span>
<span class="sd">           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it</span>
<span class="sd">           will be removed in 0.25. Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    bootstrap : bool, default=True</span>
<span class="sd">        Whether bootstrap samples are used when building trees. If False, the</span>
<span class="sd">        whole dataset is used to build each tree.</span>

<span class="sd">    oob_score : bool, default=False</span>
<span class="sd">        whether to use out-of-bag samples to estimate</span>
<span class="sd">        the R^2 on unseen data.</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,</span>
<span class="sd">        :meth:`decision_path` and :meth:`apply` are all parallelized over the</span>
<span class="sd">        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`</span>
<span class="sd">        context. ``-1`` means using all processors. See :term:`Glossary</span>
<span class="sd">        &lt;n_jobs&gt;` for more details.</span>

<span class="sd">    random_state : int or RandomState, default=None</span>
<span class="sd">        Controls both the randomness of the bootstrapping of the samples used</span>
<span class="sd">        when building trees (if ``bootstrap=True``) and the sampling of the</span>
<span class="sd">        features to consider when looking for the best split at each node</span>
<span class="sd">        (if ``max_features &lt; n_features``).</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;` for details.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">        Complexity parameter used for Minimal Cost-Complexity Pruning. The</span>
<span class="sd">        subtree with the largest cost complexity that is smaller than</span>
<span class="sd">        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See</span>
<span class="sd">        :ref:`minimal_cost_complexity_pruning` for details.</span>

<span class="sd">        .. versionadded:: 0.22</span>

<span class="sd">    max_samples : int or float, default=None</span>
<span class="sd">        If bootstrap is True, the number of samples to draw from X</span>
<span class="sd">        to train each base estimator.</span>

<span class="sd">        - If None (default), then draw `X.shape[0]` samples.</span>
<span class="sd">        - If int, then draw `max_samples` samples.</span>
<span class="sd">        - If float, then draw `max_samples * X.shape[0]` samples. Thus,</span>
<span class="sd">          `max_samples` should be in the interval `(0, 1)`.</span>

<span class="sd">        .. versionadded:: 0.22</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator_ : DecisionTreeRegressor</span>
<span class="sd">        The child estimator template used to create the collection of fitted</span>
<span class="sd">        sub-estimators.</span>

<span class="sd">    estimators_ : list of DecisionTreeRegressor</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">        The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>

<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>
<span class="sd">        This attribute exists only when ``oob_score`` is True.</span>

<span class="sd">    oob_prediction_ : ndarray of shape (n_samples,)</span>
<span class="sd">        Prediction computed with out-of-bag estimate on the training set.</span>
<span class="sd">        This attribute exists only when ``oob_score`` is True.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    DecisionTreeRegressor, ExtraTreesRegressor</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The default values for the parameters controlling the size of the trees</span>
<span class="sd">    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and</span>
<span class="sd">    unpruned trees which can potentially be very large on some data sets. To</span>
<span class="sd">    reduce memory consumption, the complexity and size of the trees should be</span>
<span class="sd">    controlled by setting those parameter values.</span>

<span class="sd">    The features are always randomly permuted at each split. Therefore,</span>
<span class="sd">    the best found split may vary, even with the same training data,</span>
<span class="sd">    ``max_features=n_features`` and ``bootstrap=False``, if the improvement</span>
<span class="sd">    of the criterion is identical for several splits enumerated during the</span>
<span class="sd">    search of the best split. To obtain a deterministic behaviour during</span>
<span class="sd">    fitting, ``random_state`` has to be fixed.</span>

<span class="sd">    The default value ``max_features=&quot;auto&quot;`` uses ``n_features``</span>
<span class="sd">    rather than ``n_features / 3``. The latter was originally suggested in</span>
<span class="sd">    [1], whereas the former was more recently justified empirically in [2].</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] L. Breiman, &quot;Random Forests&quot;, Machine Learning, 45(1), 5-32, 2001.</span>

<span class="sd">    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, &quot;Extremely randomized</span>
<span class="sd">           trees&quot;, Machine Learning, 63(1), 3-42, 2006.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import RandomForestRegressor</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_regression</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_regression(n_features=4, n_informative=2,</span>
<span class="sd">    ...                        random_state=0, shuffle=False)</span>
<span class="sd">    &gt;&gt;&gt; regr = RandomForestRegressor(max_depth=2, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; regr.fit(X, y)</span>
<span class="sd">    RandomForestRegressor(...)</span>
<span class="sd">    &gt;&gt;&gt; print(regr.predict([[0, 0, 0, 0]]))</span>
<span class="sd">    [-8.32987858]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">DecisionTreeRegressor</span><span class="p">(),</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span> <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;random_state&quot;</span><span class="p">,</span> <span class="s2">&quot;ccp_alpha&quot;</span><span class="p">),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ccp_alpha</span> <span class="o">=</span> <span class="n">ccp_alpha</span>


<span class="k">class</span> <span class="nc">ExtraTreesClassifier</span><span class="p">(</span><span class="n">ForestClassifier</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An extra-trees classifier.</span>

<span class="sd">    This class implements a meta estimator that fits a number of</span>
<span class="sd">    randomized decision trees (a.k.a. extra-trees) on various sub-samples</span>
<span class="sd">    of the dataset and uses averaging to improve the predictive accuracy</span>
<span class="sd">    and control over-fitting.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;forest&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : int, default=100</span>
<span class="sd">        The number of trees in the forest.</span>

<span class="sd">        .. versionchanged:: 0.22</span>
<span class="sd">           The default value of ``n_estimators`` changed from 10 to 100</span>
<span class="sd">           in 0.22.</span>

<span class="sd">    criterion : {&quot;gini&quot;, &quot;entropy&quot;}, default=&quot;gini&quot;</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria are</span>
<span class="sd">        &quot;gini&quot; for the Gini impurity and &quot;entropy&quot; for the information gain.</span>

<span class="sd">    max_depth : int, default=None</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : {&quot;auto&quot;, &quot;sqrt&quot;, &quot;log2&quot;}, int or float, default=&quot;auto&quot;</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float, default=None</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19. The default value of</span>
<span class="sd">           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it</span>
<span class="sd">           will be removed in 0.25. Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    bootstrap : bool, default=False</span>
<span class="sd">        Whether bootstrap samples are used when building trees. If False, the</span>
<span class="sd">        whole dataset is used to build each tree.</span>

<span class="sd">    oob_score : bool, default=False</span>
<span class="sd">        Whether to use out-of-bag samples to estimate</span>
<span class="sd">        the generalization accuracy.</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,</span>
<span class="sd">        :meth:`decision_path` and :meth:`apply` are all parallelized over the</span>
<span class="sd">        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`</span>
<span class="sd">        context. ``-1`` means using all processors. See :term:`Glossary</span>
<span class="sd">        &lt;n_jobs&gt;` for more details.</span>

<span class="sd">    random_state : int, RandomState, default=None</span>
<span class="sd">        Controls 3 sources of randomness:</span>

<span class="sd">        - the bootstrapping of the samples used when building trees</span>
<span class="sd">          (if ``bootstrap=True``)</span>
<span class="sd">        - the sampling of the features to consider when looking for the best</span>
<span class="sd">          split at each node (if ``max_features &lt; n_features``)</span>
<span class="sd">        - the draw of the splits for each of the `max_features`</span>

<span class="sd">        See :term:`Glossary &lt;random_state&gt;` for details.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    class_weight : {&quot;balanced&quot;, &quot;balanced_subsample&quot;}, dict or list of dicts, \</span>
<span class="sd">            default=None</span>
<span class="sd">        Weights associated with classes in the form ``{class_label: weight}``.</span>
<span class="sd">        If not given, all classes are supposed to have weight one. For</span>
<span class="sd">        multi-output problems, a list of dicts can be provided in the same</span>
<span class="sd">        order as the columns of y.</span>

<span class="sd">        Note that for multioutput (including multilabel) weights should be</span>
<span class="sd">        defined for each class of every column in its own dict. For example,</span>
<span class="sd">        for four-class multilabel classification weights should be</span>
<span class="sd">        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of</span>
<span class="sd">        [{1:1}, {2:5}, {3:1}, {4:1}].</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">        The &quot;balanced_subsample&quot; mode is the same as &quot;balanced&quot; except that</span>
<span class="sd">        weights are computed based on the bootstrap sample for every tree</span>
<span class="sd">        grown.</span>

<span class="sd">        For multi-output, the weights of each column of y will be multiplied.</span>

<span class="sd">        Note that these weights will be multiplied with sample_weight (passed</span>
<span class="sd">        through the fit method) if sample_weight is specified.</span>

<span class="sd">    ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">        Complexity parameter used for Minimal Cost-Complexity Pruning. The</span>
<span class="sd">        subtree with the largest cost complexity that is smaller than</span>
<span class="sd">        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See</span>
<span class="sd">        :ref:`minimal_cost_complexity_pruning` for details.</span>

<span class="sd">        .. versionadded:: 0.22</span>

<span class="sd">    max_samples : int or float, default=None</span>
<span class="sd">        If bootstrap is True, the number of samples to draw from X</span>
<span class="sd">        to train each base estimator.</span>

<span class="sd">        - If None (default), then draw `X.shape[0]` samples.</span>
<span class="sd">        - If int, then draw `max_samples` samples.</span>
<span class="sd">        - If float, then draw `max_samples * X.shape[0]` samples. Thus,</span>
<span class="sd">          `max_samples` should be in the interval `(0, 1)`.</span>

<span class="sd">        .. versionadded:: 0.22</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator_ : ExtraTreesClassifier</span>
<span class="sd">        The child estimator template used to create the collection of fitted</span>
<span class="sd">        sub-estimators.</span>

<span class="sd">    estimators_ : list of DecisionTreeClassifier</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    classes_ : ndarray of shape (n_classes,) or a list of such arrays</span>
<span class="sd">        The classes labels (single output problem), or a list of arrays of</span>
<span class="sd">        class labels (multi-output problem).</span>

<span class="sd">    n_classes_ : int or list</span>
<span class="sd">        The number of classes (single output problem), or a list containing the</span>
<span class="sd">        number of classes for each output (multi-output problem).</span>

<span class="sd">    feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">        The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>

<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features when ``fit`` is performed.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs when ``fit`` is performed.</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>
<span class="sd">        This attribute exists only when ``oob_score`` is True.</span>

<span class="sd">    oob_decision_function_ : ndarray of shape (n_samples, n_classes)</span>
<span class="sd">        Decision function computed with out-of-bag estimate on the training</span>
<span class="sd">        set. If n_estimators is small it might be possible that a data point</span>
<span class="sd">        was never left out during the bootstrap. In this case,</span>
<span class="sd">        `oob_decision_function_` might contain NaN. This attribute exists</span>
<span class="sd">        only when ``oob_score`` is True.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.</span>
<span class="sd">    RandomForestClassifier : Ensemble Classifier based on trees with optimal</span>
<span class="sd">        splits.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The default values for the parameters controlling the size of the trees</span>
<span class="sd">    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and</span>
<span class="sd">    unpruned trees which can potentially be very large on some data sets. To</span>
<span class="sd">    reduce memory consumption, the complexity and size of the trees should be</span>
<span class="sd">    controlled by setting those parameter values.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, &quot;Extremely randomized</span>
<span class="sd">           trees&quot;, Machine Learning, 63(1), 3-42, 2006.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import ExtraTreesClassifier</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import make_classification</span>
<span class="sd">    &gt;&gt;&gt; X, y = make_classification(n_features=4, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf = ExtraTreesClassifier(n_estimators=100, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y)</span>
<span class="sd">    ExtraTreesClassifier(random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; clf.predict([[0, 0, 0, 0]])</span>
<span class="sd">    array([1])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;gini&quot;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">ExtraTreeClassifier</span><span class="p">(),</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span> <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;random_state&quot;</span><span class="p">,</span> <span class="s2">&quot;ccp_alpha&quot;</span><span class="p">),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ccp_alpha</span> <span class="o">=</span> <span class="n">ccp_alpha</span>


<span class="k">class</span> <span class="nc">ExtraTreesRegressor</span><span class="p">(</span><span class="n">ForestRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An extra-trees regressor.</span>

<span class="sd">    This class implements a meta estimator that fits a number of</span>
<span class="sd">    randomized decision trees (a.k.a. extra-trees) on various sub-samples</span>
<span class="sd">    of the dataset and uses averaging to improve the predictive accuracy</span>
<span class="sd">    and control over-fitting.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;forest&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : int, default=100</span>
<span class="sd">        The number of trees in the forest.</span>

<span class="sd">        .. versionchanged:: 0.22</span>
<span class="sd">           The default value of ``n_estimators`` changed from 10 to 100</span>
<span class="sd">           in 0.22.</span>

<span class="sd">    criterion : {&quot;mse&quot;, &quot;mae&quot;}, default=&quot;mse&quot;</span>
<span class="sd">        The function to measure the quality of a split. Supported criteria</span>
<span class="sd">        are &quot;mse&quot; for the mean squared error, which is equal to variance</span>
<span class="sd">        reduction as feature selection criterion, and &quot;mae&quot; for the mean</span>
<span class="sd">        absolute error.</span>

<span class="sd">        .. versionadded:: 0.18</span>
<span class="sd">           Mean Absolute Error (MAE) criterion.</span>

<span class="sd">    max_depth : int, default=None</span>
<span class="sd">        The maximum depth of the tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` are the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_features : {&quot;auto&quot;, &quot;sqrt&quot;, &quot;log2&quot;} int or float, default=&quot;auto&quot;</span>
<span class="sd">        The number of features to consider when looking for the best split:</span>

<span class="sd">        - If int, then consider `max_features` features at each split.</span>
<span class="sd">        - If float, then `max_features` is a fraction and</span>
<span class="sd">          `int(max_features * n_features)` features are considered at each</span>
<span class="sd">          split.</span>
<span class="sd">        - If &quot;auto&quot;, then `max_features=n_features`.</span>
<span class="sd">        - If &quot;sqrt&quot;, then `max_features=sqrt(n_features)`.</span>
<span class="sd">        - If &quot;log2&quot;, then `max_features=log2(n_features)`.</span>
<span class="sd">        - If None, then `max_features=n_features`.</span>

<span class="sd">        Note: the search for a split does not stop until at least one</span>
<span class="sd">        valid partition of the node samples is found, even if it requires to</span>
<span class="sd">        effectively inspect more than ``max_features`` features.</span>

<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float, default=None</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19. The default value of</span>
<span class="sd">           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it</span>
<span class="sd">           will be removed in 0.25. Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    bootstrap : bool, default=False</span>
<span class="sd">        Whether bootstrap samples are used when building trees. If False, the</span>
<span class="sd">        whole dataset is used to build each tree.</span>

<span class="sd">    oob_score : bool, default=False</span>
<span class="sd">        Whether to use out-of-bag samples to estimate the R^2 on unseen data.</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,</span>
<span class="sd">        :meth:`decision_path` and :meth:`apply` are all parallelized over the</span>
<span class="sd">        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`</span>
<span class="sd">        context. ``-1`` means using all processors. See :term:`Glossary</span>
<span class="sd">        &lt;n_jobs&gt;` for more details.</span>

<span class="sd">    random_state : int or RandomState, default=None</span>
<span class="sd">        Controls 3 sources of randomness:</span>

<span class="sd">        - the bootstrapping of the samples used when building trees</span>
<span class="sd">          (if ``bootstrap=True``)</span>
<span class="sd">        - the sampling of the features to consider when looking for the best</span>
<span class="sd">          split at each node (if ``max_features &lt; n_features``)</span>
<span class="sd">        - the draw of the splits for each of the `max_features`</span>

<span class="sd">        See :term:`Glossary &lt;random_state&gt;` for details.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">        Complexity parameter used for Minimal Cost-Complexity Pruning. The</span>
<span class="sd">        subtree with the largest cost complexity that is smaller than</span>
<span class="sd">        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See</span>
<span class="sd">        :ref:`minimal_cost_complexity_pruning` for details.</span>

<span class="sd">        .. versionadded:: 0.22</span>

<span class="sd">    max_samples : int or float, default=None</span>
<span class="sd">        If bootstrap is True, the number of samples to draw from X</span>
<span class="sd">        to train each base estimator.</span>

<span class="sd">        - If None (default), then draw `X.shape[0]` samples.</span>
<span class="sd">        - If int, then draw `max_samples` samples.</span>
<span class="sd">        - If float, then draw `max_samples * X.shape[0]` samples. Thus,</span>
<span class="sd">          `max_samples` should be in the interval `(0, 1)`.</span>

<span class="sd">        .. versionadded:: 0.22</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    base_estimator_ : ExtraTreeRegressor</span>
<span class="sd">        The child estimator template used to create the collection of fitted</span>
<span class="sd">        sub-estimators.</span>

<span class="sd">    estimators_ : list of DecisionTreeRegressor</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    feature_importances_ : ndarray of shape (n_features,)</span>
<span class="sd">        The impurity-based feature importances.</span>
<span class="sd">        The higher, the more important the feature.</span>
<span class="sd">        The importance of a feature is computed as the (normalized)</span>
<span class="sd">        total reduction of the criterion brought by that feature.  It is also</span>
<span class="sd">        known as the Gini importance.</span>

<span class="sd">        Warning: impurity-based feature importances can be misleading for</span>
<span class="sd">        high cardinality features (many unique values). See</span>
<span class="sd">        :func:`sklearn.inspection.permutation_importance` as an alternative.</span>

<span class="sd">    n_features_ : int</span>
<span class="sd">        The number of features.</span>

<span class="sd">    n_outputs_ : int</span>
<span class="sd">        The number of outputs.</span>

<span class="sd">    oob_score_ : float</span>
<span class="sd">        Score of the training dataset obtained using an out-of-bag estimate.</span>
<span class="sd">        This attribute exists only when ``oob_score`` is True.</span>

<span class="sd">    oob_prediction_ : ndarray of shape (n_samples,)</span>
<span class="sd">        Prediction computed with out-of-bag estimate on the training set.</span>
<span class="sd">        This attribute exists only when ``oob_score`` is True.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.</span>
<span class="sd">    RandomForestRegressor: Ensemble regressor using trees with optimal splits.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The default values for the parameters controlling the size of the trees</span>
<span class="sd">    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and</span>
<span class="sd">    unpruned trees which can potentially be very large on some data sets. To</span>
<span class="sd">    reduce memory consumption, the complexity and size of the trees should be</span>
<span class="sd">    controlled by setting those parameter values.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, &quot;Extremely randomized trees&quot;,</span>
<span class="sd">           Machine Learning, 63(1), 3-42, 2006.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.datasets import load_diabetes</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.model_selection import train_test_split</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import ExtraTreesRegressor</span>
<span class="sd">    &gt;&gt;&gt; X, y = load_diabetes(return_X_y=True)</span>
<span class="sd">    &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(</span>
<span class="sd">    ...     X, y, random_state=0)</span>
<span class="sd">    &gt;&gt;&gt; reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(</span>
<span class="sd">    ...    X_train, y_train)</span>
<span class="sd">    &gt;&gt;&gt; reg.score(X_test, y_test)</span>
<span class="sd">    0.2708...</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ccp_alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">ExtraTreeRegressor</span><span class="p">(),</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span> <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;random_state&quot;</span><span class="p">,</span> <span class="s2">&quot;ccp_alpha&quot;</span><span class="p">),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ccp_alpha</span> <span class="o">=</span> <span class="n">ccp_alpha</span>


<span class="k">class</span> <span class="nc">RandomTreesEmbedding</span><span class="p">(</span><span class="n">BaseForest</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An ensemble of totally random trees.</span>

<span class="sd">    An unsupervised transformation of a dataset to a high-dimensional</span>
<span class="sd">    sparse representation. A datapoint is coded according to which leaf of</span>
<span class="sd">    each tree it is sorted into. Using a one-hot encoding of the leaves,</span>
<span class="sd">    this leads to a binary coding with as many ones as there are trees in</span>
<span class="sd">    the forest.</span>

<span class="sd">    The dimensionality of the resulting representation is</span>
<span class="sd">    ``n_out &lt;= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,</span>
<span class="sd">    the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;random_trees_embedding&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_estimators : int, default=100</span>
<span class="sd">        Number of trees in the forest.</span>

<span class="sd">        .. versionchanged:: 0.22</span>
<span class="sd">           The default value of ``n_estimators`` changed from 10 to 100</span>
<span class="sd">           in 0.22.</span>

<span class="sd">    max_depth : int, default=5</span>
<span class="sd">        The maximum depth of each tree. If None, then nodes are expanded until</span>
<span class="sd">        all leaves are pure or until all leaves contain less than</span>
<span class="sd">        min_samples_split samples.</span>

<span class="sd">    min_samples_split : int or float, default=2</span>
<span class="sd">        The minimum number of samples required to split an internal node:</span>

<span class="sd">        - If int, then consider `min_samples_split` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_split` is a fraction and</span>
<span class="sd">          `ceil(min_samples_split * n_samples)` is the minimum</span>
<span class="sd">          number of samples for each split.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_samples_leaf : int or float, default=1</span>
<span class="sd">        The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        A split point at any depth will only be considered if it leaves at</span>
<span class="sd">        least ``min_samples_leaf`` training samples in each of the left and</span>
<span class="sd">        right branches.  This may have the effect of smoothing the model,</span>
<span class="sd">        especially in regression.</span>

<span class="sd">        - If int, then consider `min_samples_leaf` as the minimum number.</span>
<span class="sd">        - If float, then `min_samples_leaf` is a fraction and</span>
<span class="sd">          `ceil(min_samples_leaf * n_samples)` is the minimum</span>
<span class="sd">          number of samples for each node.</span>

<span class="sd">        .. versionchanged:: 0.18</span>
<span class="sd">           Added float values for fractions.</span>

<span class="sd">    min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">        The minimum weighted fraction of the sum total of weights (of all</span>
<span class="sd">        the input samples) required to be at a leaf node. Samples have</span>
<span class="sd">        equal weight when sample_weight is not provided.</span>

<span class="sd">    max_leaf_nodes : int, default=None</span>
<span class="sd">        Grow trees with ``max_leaf_nodes`` in best-first fashion.</span>
<span class="sd">        Best nodes are defined as relative reduction in impurity.</span>
<span class="sd">        If None then unlimited number of leaf nodes.</span>

<span class="sd">    min_impurity_decrease : float, default=0.0</span>
<span class="sd">        A node will be split if this split induces a decrease of the impurity</span>
<span class="sd">        greater than or equal to this value.</span>

<span class="sd">        The weighted impurity decrease equation is the following::</span>

<span class="sd">            N_t / N * (impurity - N_t_R / N_t * right_impurity</span>
<span class="sd">                                - N_t_L / N_t * left_impurity)</span>

<span class="sd">        where ``N`` is the total number of samples, ``N_t`` is the number of</span>
<span class="sd">        samples at the current node, ``N_t_L`` is the number of samples in the</span>
<span class="sd">        left child, and ``N_t_R`` is the number of samples in the right child.</span>

<span class="sd">        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,</span>
<span class="sd">        if ``sample_weight`` is passed.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    min_impurity_split : float, default=None</span>
<span class="sd">        Threshold for early stopping in tree growth. A node will split</span>
<span class="sd">        if its impurity is above the threshold, otherwise it is a leaf.</span>

<span class="sd">        .. deprecated:: 0.19</span>
<span class="sd">           ``min_impurity_split`` has been deprecated in favor of</span>
<span class="sd">           ``min_impurity_decrease`` in 0.19. The default value of</span>
<span class="sd">           ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it</span>
<span class="sd">           will be removed in 0.25. Use ``min_impurity_decrease`` instead.</span>

<span class="sd">    sparse_output : bool, default=True</span>
<span class="sd">        Whether or not to return a sparse CSR matrix, as default behavior,</span>
<span class="sd">        or to return a dense array compatible with dense pipeline operators.</span>

<span class="sd">    n_jobs : int, default=None</span>
<span class="sd">        The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,</span>
<span class="sd">        :meth:`decision_path` and :meth:`apply` are all parallelized over the</span>
<span class="sd">        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`</span>
<span class="sd">        context. ``-1`` means using all processors. See :term:`Glossary</span>
<span class="sd">        &lt;n_jobs&gt;` for more details.</span>

<span class="sd">    random_state : int or RandomState, default=None</span>
<span class="sd">        Controls the generation of the random `y` used to fit the trees</span>
<span class="sd">        and the draw of the splits for each feature at the trees&#39; nodes.</span>
<span class="sd">        See :term:`Glossary &lt;random_state&gt;` for details.</span>

<span class="sd">    verbose : int, default=0</span>
<span class="sd">        Controls the verbosity when fitting and predicting.</span>

<span class="sd">    warm_start : bool, default=False</span>
<span class="sd">        When set to ``True``, reuse the solution of the previous call to fit</span>
<span class="sd">        and add more estimators to the ensemble, otherwise, just fit a whole</span>
<span class="sd">        new forest. See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    estimators_ : list of DecisionTreeClassifier</span>
<span class="sd">        The collection of fitted sub-estimators.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] P. Geurts, D. Ernst., and L. Wehenkel, &quot;Extremely randomized trees&quot;,</span>
<span class="sd">           Machine Learning, 63(1), 3-42, 2006.</span>
<span class="sd">    .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  &quot;Fast discriminative</span>
<span class="sd">           visual codebooks using randomized clustering forests&quot;</span>
<span class="sd">           NIPS 2007</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; from sklearn.ensemble import RandomTreesEmbedding</span>
<span class="sd">    &gt;&gt;&gt; X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]</span>
<span class="sd">    &gt;&gt;&gt; random_trees = RandomTreesEmbedding(</span>
<span class="sd">    ...    n_estimators=5, random_state=0, max_depth=1).fit(X)</span>
<span class="sd">    &gt;&gt;&gt; X_sparse_embedding = random_trees.transform(X)</span>
<span class="sd">    &gt;&gt;&gt; X_sparse_embedding.toarray()</span>
<span class="sd">    array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],</span>
<span class="sd">           [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],</span>
<span class="sd">           [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],</span>
<span class="sd">           [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],</span>
<span class="sd">           [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="s1">&#39;mse&#39;</span>
    <span class="n">max_features</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="nd">@_deprecate_positional_args</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
                 <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">min_impurity_decrease</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">min_impurity_split</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">sparse_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">base_estimator</span><span class="o">=</span><span class="n">ExtraTreeRegressor</span><span class="p">(),</span>
            <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
            <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;min_impurity_decrease&quot;</span><span class="p">,</span> <span class="s2">&quot;min_impurity_split&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;random_state&quot;</span><span class="p">),</span>
            <span class="n">bootstrap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">max_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_decrease</span> <span class="o">=</span> <span class="n">min_impurity_decrease</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_impurity_split</span> <span class="o">=</span> <span class="n">min_impurity_split</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_output</span> <span class="o">=</span> <span class="n">sparse_output</span>

    <span class="k">def</span> <span class="nf">_set_oob_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;OOB score not supported by tree embedding&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit estimator.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            The input samples. Use ``dtype=np.float32`` for maximum</span>
<span class="sd">            efficiency. Sparse matrices are also supported, use sparse</span>
<span class="sd">            ``csc_matrix`` for maximum efficiency.</span>

<span class="sd">        y : Ignored</span>
<span class="sd">            Not used, present for API consistency by convention.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. In the case of</span>
<span class="sd">            classification, splits are also ignored if they would result in any</span>
<span class="sd">            single class carrying a negative weight in either child node.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit estimator and transform dataset.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Input data used to build forests. Use ``dtype=np.float32`` for</span>
<span class="sd">            maximum efficiency.</span>

<span class="sd">        y : Ignored</span>
<span class="sd">            Not used, present for API consistency by convention.</span>

<span class="sd">        sample_weight : array-like of shape (n_samples,), default=None</span>
<span class="sd">            Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">            that would create child nodes with net zero or negative weight are</span>
<span class="sd">            ignored while searching for a split in each node. In the case of</span>
<span class="sd">            classification, splits are also ignored if they would result in any</span>
<span class="sd">            single class carrying a negative weight in either child node.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_transformed : sparse matrix of shape (n_samples, n_out)</span>
<span class="sd">            Transformed dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;csc&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="c1"># Pre-sort indices to avoid that each individual tree of the</span>
            <span class="c1"># ensemble sorts the indices.</span>
            <span class="n">X</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>

        <span class="n">rnd</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">rnd</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">one_hot_encoder_</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sparse_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_hot_encoder_</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transform dataset.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span>
<span class="sd">            Input data to be transformed. Use ``dtype=np.float32`` for maximum</span>
<span class="sd">            efficiency. Sparse matrices are also supported, use sparse</span>
<span class="sd">            ``csr_matrix`` for maximum efficiency.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        X_transformed : sparse matrix of shape (n_samples, n_out)</span>
<span class="sd">            Transformed dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_hot_encoder_</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>

      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2017 - 2020, scikit-optimize contributors (BSD License).
      </footer>
    </div>
  </div>
</div>
<script src="../../../_static/js/vendor/bootstrap.min.js"></script>


<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code sampler to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
    <script src="https://scikit-optimize.github.io/versionwarning.js"></script>
</body>
</html>